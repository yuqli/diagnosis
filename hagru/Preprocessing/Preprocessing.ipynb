{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: this notebook takes a very, very long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "from nltk import word_tokenize, sent_tokenize, text\n",
    "from collections import Counter\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from sklearn import feature_extraction\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    '''\n",
    "    This function takes a string of text and does the following:\n",
    "    - Remove initial \"*\" characters -- these are useless and extra noise\n",
    "    - Convert alphabetic characters to lower case (Hello --> hello)\n",
    "    - Replace numeric characters with \"#\" character.\n",
    "    '''\n",
    "    text = text.replace('*','').lower()\n",
    "    text = re.sub('\\d', '*', text)\n",
    "    return text\n",
    "\n",
    "def assign_numbers(vocab_list):\n",
    "    '''\n",
    "    This function assigns index numbers to words in a vocabulary.\n",
    "    Note that \"1\" is reserved for unknown words, \"0\" for paddings.\n",
    "    '''\n",
    "    vocab_indexer = dict()\n",
    "    index_value = 2\n",
    "    for word in vocab_list:\n",
    "        vocab_indexer[word] = index_value\n",
    "        index_value = index_value+1\n",
    "    return vocab_indexer\n",
    "\n",
    "def create_vocabulary(counter, threshold):\n",
    "    '''\n",
    "    @counter is a Counter object listing the number of times each token appears in the corpus.\n",
    "    @threshold = number of times a token must appear in corpus to be listed in vocabulary. \n",
    "    '''\n",
    "    Vocabulary = []\n",
    "    for key in counter:\n",
    "        value = counter[key]\n",
    "        if value>=threshold:\n",
    "            Vocabulary.append(key)\n",
    "    return assign_numbers(Vocabulary)\n",
    "\n",
    "def token_to_index(tokens, token_indexer, list_format=False):\n",
    "    \"\"\"\n",
    "    Function that transforms a list of tokens in a document and coverts it to a list of token indices.\n",
    "    @param tokens: list of tokens from one document\n",
    "    @param token_indexer: dictionary that maps each token in the vocabulary to an unique index\n",
    "    @param not_list: if True, then items in token_indexer are Lists (relevant for byte encoder case). \n",
    "    \n",
    "    \"\"\"\n",
    "    # Please DO NOT assign any ngram in the vocabulary to index 0\n",
    "    document = []\n",
    "    for token in tokens:\n",
    "        if list_format == True:\n",
    "            try:\n",
    "                document.extend(token_indexer[token])\n",
    "            except:\n",
    "                document.append(1)\n",
    "\n",
    "        elif list_format == False:\n",
    "            try:  \n",
    "                document.append(token_indexer[token])\n",
    "            except:\n",
    "                document.append(1)\n",
    "    return document\n",
    "\n",
    "\n",
    "def sentence_to_indexes(sentences, token_indexer, list_format=False):\n",
    "    \"\"\"\n",
    "    Function that transforms a list of sentences of WORDS in a document and coverts it to a list of \n",
    "    sentences of INDEXES.\n",
    "    \n",
    "    @param sentences: list of sentences from one document\n",
    "    @param token_indexer: dictionary that maps each token in the vocabulary to an unique index\n",
    "    \n",
    "    @param not_list: if True, then items in token_indexer are Lists (relevant for byte encoder case). \n",
    "    \n",
    "    \"\"\"\n",
    "    # Please DO NOT assign any ngram in the vocabulary to index 0\n",
    "    document = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence) #tokenize each individual sentence to unigrams\n",
    "        representation = token_to_index(tokens, token_indexer, list_format=list_format)\n",
    "        #Each sentence is now a list of indexed tokens. \n",
    "        document.append(representation)\n",
    "    return document\n",
    "\n",
    "\n",
    "#-------------------------- For Byte-Pair Encoding method\n",
    "\n",
    "def split_tokens_into_characters(counter):\n",
    "    '''This function takes a counter (indicating the counts per token across the entire corpus) and \n",
    "    splits each token into a series of characters'''\n",
    "    char_dict = {}\n",
    "    old_dict = dict(counter)\n",
    "    for token in old_dict.keys():\n",
    "        char_form = \" \".join(token)+' </w>' #add ending character\n",
    "        value = old_dict[token]\n",
    "        char_dict[char_form] = value \n",
    "    return char_dict\n",
    "\n",
    "def get_stats(vocab):\n",
    "    '''This function counts the number of times each pair of characters appears next to each other.\n",
    "    For example the sentence \"baking a cake\" would return (a,k):2, (b,a):1, (k,e):1, and so on.\n",
    "    '''\n",
    "    pairs = collections.defaultdict(int) \n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq \n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    '''This function changes the vocabulary so that n-grams that co-occur together frequently\n",
    "    are merged'''\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word] \n",
    "    return v_out\n",
    "\n",
    "def convert_tokens_into_frequent_character_combinations(counter, num_merges, ongoing = False):\n",
    "    '''This function takes a Counter object (counts per token) and converts it so that \n",
    "    the tokens (keys) become split into frequent character combinations.\n",
    "    For example, {'cake': 3, 'baking': 1} might become {'c ak e': 3, 'b ak ing': 1}\n",
    "    \n",
    "    @num_merges is the number of \"rounds\" for merging character blocks. For each round, the \n",
    "    pair of characters that co-occur the most are combined. \n",
    "    \n",
    "    @num_merges is a very important hyperparameter.\n",
    "    \n",
    "    @ongoing = False if you are starting from 0 merges. However, for efficiency reasons, it often\n",
    "    makes sense to do 10,000 merges, then a subsequent 10,000 merges later on, rather than all \n",
    "    20,000 merges at one time. \n",
    "    '''\n",
    "    if ongoing == False:\n",
    "        char_dict = split_tokens_into_characters(counter)\n",
    "    else:\n",
    "        char_dict = counter.copy()\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        pairs = get_stats(char_dict)\n",
    "        try:\n",
    "            del pairs[('\\\\**', '\\\\')] #bugs\n",
    "            del pairs[('*', '\\\\')]\n",
    "        except:\n",
    "            pass\n",
    "        best = max(pairs, key=pairs.get) \n",
    "        char_dict = merge_vocab(best, char_dict)\n",
    "    return char_dict\n",
    "\n",
    "def create_vocabulary_of_character_combos(char_dict):\n",
    "    '''Takes the character_dictionary from above and creates an indexed vocabulary \n",
    "    for frequent character combinations.\n",
    "    \n",
    "    For example, {'c ak e': 3, 'b ak ing': 1} would become something like\n",
    "    {'c': 1, 'b': 2, 'ak': 3, 'e': 4, 'ing': 5}. \n",
    "    \n",
    "    Eventually we want to map each unigram to a series of character blocks \n",
    "    (e.g. 'baking' becomes 2 3 5)\n",
    "    '''\n",
    "    #First, find the set of all grams\n",
    "    grams = []\n",
    "    for word in char_dict.keys():\n",
    "        word_grams = word.split(' ')\n",
    "        grams.extend(word_grams)\n",
    "    GRAMS = list(np.unique(grams)) #Delete any duplicates\n",
    "    Vocab_Index = assign_numbers(GRAMS)\n",
    "    return Vocab_Index\n",
    "\n",
    "def map_unigram_to_character_indices(char_dict, Vocab_Index):\n",
    "    '''This function maps each unigram token to a series of numbers. Each number corresponds to a \n",
    "    character block.\n",
    "    \n",
    "    For example, if Vocab_Index = {'c': 1, 'b': 2, 'ak': 3, 'e': 4, 'ing': 5}, \n",
    "    \"baking\" should be mapped to [2, 3, 5]\n",
    "    \n",
    "    \n",
    "    @char_dict is the counter-like object where the keys are unigrams split into character blocks, \n",
    "    values are the number of times the unigram appears.\n",
    "    '''\n",
    "    unigrams = {} \n",
    "    for x in char_dict.keys():\n",
    "        unigram = x.replace(' ','').replace('</w>','') #e.g. revert \"b ak ing<w>\" to baking\n",
    "        unigrams[unigram] = x\n",
    "    \n",
    "    for w in unigrams.keys():\n",
    "        unigrams[w] = token_to_index(unigrams[w].split(' '), Vocab_Index)\n",
    "    return unigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_as_pkl(vocab, filename):\n",
    "    '''Function takes a file and saves it as a pkl file'''\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "\n",
    "def load_pkl(filename):\n",
    "    '''This function loads a pkl file'''\n",
    "    with open(filename, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will only train the model on records that appear in MIMIC II, due to time and space constraints. \n",
    "\n",
    "### Determine which SUBJ_IDs should be in the training vs. test sets\n",
    "Most studies with MIMIC text data have used the training/test split employed by Perrotte et al (2014) for MIMIC II. Please refer to https://physionet.org/works/ICD9CodingofDischargeSummaries/\n",
    "\n",
    "\n",
    "Using the code provided in the link above (with some small edits), we found lists of Subject_IDs that belong to the training set and the test set. Note that the HADM_IDs in MIMIC II are completely different from the HADMIDs in MIMIC III, so we needed to use SUBJECT_ID as the primary key.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = './Perotte code/'\n",
    "\n",
    "#Get list of SubjectIDs associated with test set\n",
    "test_subj_ids = []\n",
    "with open(path+'testing_SUBJ_IDs.data') as fin:\n",
    "    for line in fin:\n",
    "        test_subj_ids.append(line.strip('\\n'))\n",
    "\n",
    "#Get list of SubjectIDs associated with MIMIC II training set\n",
    "train_subj_ids = []\n",
    "with open(path+'training_SUBJ_IDs.data') as fin:\n",
    "    for line in fin:\n",
    "        train_subj_ids.append(line.strip('\\n'))\n",
    "train_subj_ids.remove('\"subject_id\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary that links each HADM_ID (hospital admission ID) to a list of ICD-9 Codes. \n",
    "\n",
    "#### NOTE: we will primarily consider the \"rolled-up\" ICD-9 codes. (Most codes are five digits but they can be \"rolled\" to a simpler three-digit code).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ICD-9 codes: 5431 \n",
      "Unique 3-Digit Codes: 970  eventually reduced to 936 after eliminating codes that appear only in test set\n",
      "HADM_IDs 30050 \n",
      "SUBJECT_IDs 21685\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>SEQ_NUM</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>Rolled_ICD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1297</td>\n",
       "      <td>109</td>\n",
       "      <td>172335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40301</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1298</td>\n",
       "      <td>109</td>\n",
       "      <td>172335</td>\n",
       "      <td>2.0</td>\n",
       "      <td>486</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1299</td>\n",
       "      <td>109</td>\n",
       "      <td>172335</td>\n",
       "      <td>3.0</td>\n",
       "      <td>58281</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1300</td>\n",
       "      <td>109</td>\n",
       "      <td>172335</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5855</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1301</td>\n",
       "      <td>109</td>\n",
       "      <td>172335</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4254</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_ID  SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE Rolled_ICD\n",
       "0    1297         109   172335      1.0     40301        403\n",
       "1    1298         109   172335      2.0       486        486\n",
       "2    1299         109   172335      3.0     58281        582\n",
       "3    1300         109   172335      4.0      5855        585\n",
       "4    1301         109   172335      5.0      4254        425"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Diagnoses_ICD = pd.read_csv('../MIMIC-III/DIAGNOSES_ICD.csv')\n",
    "Diagnoses_ICD = Diagnoses_ICD[-Diagnoses_ICD['ICD9_CODE'].isnull()]\n",
    "\n",
    "#Only consider rows that appear in MIMIC II\n",
    "Diagnoses_ICD = Diagnoses_ICD[(Diagnoses_ICD['SUBJECT_ID'].isin(train_subj_ids))|\n",
    "                             (Diagnoses_ICD['SUBJECT_ID'].isin(test_subj_ids))]\n",
    "\n",
    "Diagnoses_ICD['Rolled_ICD'] = np.where(Diagnoses_ICD['ICD9_CODE'].str[0] == 'E',\n",
    "                                       Diagnoses_ICD['ICD9_CODE'].str[0:4],\n",
    "                                       Diagnoses_ICD['ICD9_CODE'].str[0:3])\n",
    "                                       \n",
    "NumberCodes = len(Diagnoses_ICD['ICD9_CODE'].unique())\n",
    "NumberRolled = len(Diagnoses_ICD['Rolled_ICD'].unique())\n",
    "\n",
    "print('Unique ICD-9 codes:', NumberCodes, '\\nUnique 3-Digit Codes:',NumberRolled, ' eventually reduced to 936 after eliminating codes that appear only in test set')\n",
    "print('HADM_IDs', len(Diagnoses_ICD['HADM_ID'].unique()), '\\nSUBJECT_IDs', \n",
    "                     len(Diagnoses_ICD['SUBJECT_ID'].unique()))\n",
    "\n",
    "Diagnoses_ICD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['276',\n",
       " '285',\n",
       " '286',\n",
       " '403',\n",
       " '410',\n",
       " '414',\n",
       " '424',\n",
       " '428',\n",
       " '458',\n",
       " '486',\n",
       " '564',\n",
       " '585',\n",
       " '785']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dictionary where each key is a HADM_ID, each value is a list of rolled-up ICD9 codes\n",
    "HADMID_Code_Dict = dict()\n",
    "Unique_Visits = Diagnoses_ICD['HADM_ID'].unique()\n",
    "\n",
    "for visit in Unique_Visits:\n",
    "    VisitDF = Diagnoses_ICD[Diagnoses_ICD['HADM_ID']==visit].reset_index()\n",
    "    ListOfICDs=[]\n",
    "    for i in range(len(VisitDF)):\n",
    "        ListOfICDs.append(VisitDF.loc[i, 'Rolled_ICD'])\n",
    "    UniqueICDs = np.unique(ListOfICDs) #For rolled ICDs\n",
    "    #ICDs = ' '.join(UniqueICDs)\n",
    "    HADMID_Code_Dict[visit] = list(UniqueICDs)\n",
    "\n",
    "HADMID_Code_Dict[100095]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidebar: Create 1-to-1 mapping between HADM_ID and SUBJECT_ID.  This will be useful in determining whether a record belongs in the training or test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>21036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>6138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      HADM_ID\n",
       "Test         \n",
       "0.0     21036\n",
       "0.5      6138\n",
       "1.0      2876"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDs = Diagnoses_ICD[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "IDs['Test'] = np.where(IDs['SUBJECT_ID'].isin(test_subj_ids), 1, 0)\n",
    "IDs = IDs.sort_values(by='Test').reset_index(drop=True)\n",
    "NumTrain = round(0.70*len(IDs))\n",
    "IDs['Test'] = np.where((IDs.index > NumTrain)&(IDs['Test']==0), 0.5, IDs['Test'])\n",
    "IDs = IDs.drop('SUBJECT_ID', 1)\n",
    "IDs.groupby('Test').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_Codes</th>\n",
       "      <th>ICD9_Codes_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100006</td>\n",
       "      <td>[203, 276, 309, 486, 493, 518, 785, V12, V15]</td>\n",
       "      <td>203 276 309 486 493 518 785 V12 V15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100007</td>\n",
       "      <td>[401, 486, 557, 560, 997]</td>\n",
       "      <td>401 486 557 560 997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100009</td>\n",
       "      <td>[250, 272, 278, 285, 401, 411, 414, 426, 440, ...</td>\n",
       "      <td>250 272 278 285 401 411 414 426 440 996 V15 V4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100014</td>\n",
       "      <td>[278, 300, 718, 726, 738, V45]</td>\n",
       "      <td>278 300 718 726 738 V45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100020</td>\n",
       "      <td>[041, 276, 293, 337, 340, 344, 345, 369, 401, ...</td>\n",
       "      <td>041 276 293 337 340 344 345 369 401 428 530 56...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID                                         ICD9_Codes  \\\n",
       "0   100006      [203, 276, 309, 486, 493, 518, 785, V12, V15]   \n",
       "1   100007                          [401, 486, 557, 560, 997]   \n",
       "2   100009  [250, 272, 278, 285, 401, 411, 414, 426, 440, ...   \n",
       "3   100014                     [278, 300, 718, 726, 738, V45]   \n",
       "4   100020  [041, 276, 293, 337, 340, 344, 345, 369, 401, ...   \n",
       "\n",
       "                                      ICD9_Codes_str  \n",
       "0                203 276 309 486 493 518 785 V12 V15  \n",
       "1                                401 486 557 560 997  \n",
       "2  250 272 278 285 401 411 414 426 440 996 V15 V4...  \n",
       "3                            278 300 718 726 738 V45  \n",
       "4  041 276 293 337 340 344 345 369 401 428 530 56...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HADMID_Code_DF = pd.DataFrame(pd.Series(HADMID_Code_Dict)).\\\n",
    "                reset_index().rename(columns={0: 'ICD9_Codes','index': 'HADM_ID'})\n",
    "    \n",
    "HADMID_Code_DF['ICD9_Codes_str'] = HADMID_Code_DF['ICD9_Codes'].map(lambda x: ' '.join(x))\n",
    "HADMID_Code_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dictionary that links each HADM_ID to a Discharge Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Brenton/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22532</td>\n",
       "      <td>167853.0</td>\n",
       "      <td>2151-08-04</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13702</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>2118-06-14</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13702</td>\n",
       "      <td>167118.0</td>\n",
       "      <td>2119-05-25</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2119-5-4**]              D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13702</td>\n",
       "      <td>196489.0</td>\n",
       "      <td>2124-08-18</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2124-7-21**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26880</td>\n",
       "      <td>135453.0</td>\n",
       "      <td>2162-03-25</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-3-3**]              D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID   HADM_ID   CHARTDATE DESCRIPTION  \\\n",
       "0       22532  167853.0  2151-08-04      Report   \n",
       "1       13702  107527.0  2118-06-14      Report   \n",
       "2       13702  167118.0  2119-05-25      Report   \n",
       "3       13702  196489.0  2124-08-18      Report   \n",
       "4       26880  135453.0  2162-03-25      Report   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
       "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
       "2  Admission Date:  [**2119-5-4**]              D...  \n",
       "3  Admission Date:  [**2124-7-21**]              ...  \n",
       "4  Admission Date:  [**2162-3-3**]              D...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoteEvents = pd.read_csv('../MIMIC-III/NOTEEVENTS.csv')\n",
    "Notes = NoteEvents[NoteEvents['CATEGORY'] == 'Discharge summary'].reset_index(drop=True)\n",
    "Notes = Notes[['SUBJECT_ID','HADM_ID','CHARTDATE','DESCRIPTION', 'TEXT']]\n",
    "Notes = Notes[(Notes['SUBJECT_ID'].isin(train_subj_ids))|\n",
    "                (Notes['SUBJECT_ID'].isin(test_subj_ids))]\n",
    "\n",
    "#Dummy dataset\n",
    "#Notes = Notes[Notes['HADM_ID']<100400]\n",
    "Notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate discharage summaries for each HADM_ID. \n",
    "An HADM_ID might have one \"Report\" and several \"Addendums\". Try to make it so that each HADM_ID is associated with EXACTLY one block of text. In order to do so, we'll need to concatenate texts for each HADM_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29685 29685\n"
     ]
    }
   ],
   "source": [
    "NotesPerID = Notes.groupby('HADM_ID').count()[['TEXT']]\n",
    "\n",
    "#HADM_IDs_with_MultipleCounts: the HADM_IDs with more than one discharge summary.\n",
    "HADM_IDs_with_MultipleCounts = NotesPerID[NotesPerID['TEXT'] > 1].index\n",
    "\n",
    "#NotesToAppend: rows of text that need to be merged according to HADM_ID\n",
    "NotesToAppend = Notes[Notes['HADM_ID'].isin(HADM_IDs_with_MultipleCounts)]\n",
    "\n",
    "UnchangedNotes = Notes[-Notes['HADM_ID'].isin(HADM_IDs_with_MultipleCounts)]\n",
    "\n",
    "ChangedNotes = pd.DataFrame()\n",
    "for ID in HADM_IDs_with_MultipleCounts: #For each HADM_ID with multiple texts\n",
    "    subdf = NotesToAppend[NotesToAppend['HADM_ID'] == ID] #create a smaller df with only that HADMID\n",
    "    combined_text = subdf['TEXT'].str.cat() #combine all text in that column into one entry \n",
    "    subdf = subdf.drop_duplicates(subset='HADM_ID') #turn smaller df into 1-row df\n",
    "    subdf['TEXT'] = combined_text \n",
    "    ChangedNotes = ChangedNotes.append(subdf)\n",
    "\n",
    "TotalNotes = ChangedNotes.append(UnchangedNotes).reset_index(drop=True)\n",
    "print(len(TotalNotes), len(TotalNotes['HADM_ID'].unique())) #now a 1-1 relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with IDs column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TotalNotes = pd.merge(IDs, TotalNotes, on='HADM_ID' )\n",
    "TotalNotes = TotalNotes.sort_values(by=['Test', 'HADM_ID']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Summaries. \n",
    "## Then tokenize text into both I) words and II) sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Steps\n",
    "- Convert all alphabetical characters to lower case\n",
    "- Eliminate any star(*) characters that do not correspond to numbers\n",
    "- Convert numeric characters to star(*) -- so \"2/2/2011\" becomes \"star/star/4stars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>Test</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>100007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23018</td>\n",
       "      <td>2145-04-07</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2145-3-31**]              ...</td>\n",
       "      <td>[admission, date, :, [, ****-*-**, ], discharg...</td>\n",
       "      <td>[admission date:  [****-*-**]              dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18269</th>\n",
       "      <td>100009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>533</td>\n",
       "      <td>2162-05-21</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2162-5-16**]              ...</td>\n",
       "      <td>[admission, date, :, [, ****-*-**, ], discharg...</td>\n",
       "      <td>[admission date:  [****-*-**]              dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15062</th>\n",
       "      <td>100031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6892</td>\n",
       "      <td>2140-11-24</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2140-11-11**]       Discha...</td>\n",
       "      <td>[admission, date, :, [, ****-**-**, ], dischar...</td>\n",
       "      <td>[admission date:  [****-**-**]       discharge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>100038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21234</td>\n",
       "      <td>2127-07-13</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2127-7-11**]              ...</td>\n",
       "      <td>[admission, date, :, [, ****-*-**, ], discharg...</td>\n",
       "      <td>[admission date:  [****-*-**]              dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17412</th>\n",
       "      <td>100045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1569</td>\n",
       "      <td>2176-02-15</td>\n",
       "      <td>Report</td>\n",
       "      <td>Admission Date:  [**2176-2-5**]              D...</td>\n",
       "      <td>[admission, date, :, [, ****-*-*, ], discharge...</td>\n",
       "      <td>[admission date:  [****-*-*]              disc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      HADM_ID  Test  SUBJECT_ID   CHARTDATE DESCRIPTION  \\\n",
       "565    100007   0.0       23018  2145-04-07      Report   \n",
       "18269  100009   0.0         533  2162-05-21      Report   \n",
       "15062  100031   0.0        6892  2140-11-24      Report   \n",
       "1383   100038   0.0       21234  2127-07-13      Report   \n",
       "17412  100045   0.0        1569  2176-02-15      Report   \n",
       "\n",
       "                                                    TEXT  \\\n",
       "565    Admission Date:  [**2145-3-31**]              ...   \n",
       "18269  Admission Date:  [**2162-5-16**]              ...   \n",
       "15062  Admission Date:  [**2140-11-11**]       Discha...   \n",
       "1383   Admission Date:  [**2127-7-11**]              ...   \n",
       "17412  Admission Date:  [**2176-2-5**]              D...   \n",
       "\n",
       "                                                  Tokens  \\\n",
       "565    [admission, date, :, [, ****-*-**, ], discharg...   \n",
       "18269  [admission, date, :, [, ****-*-**, ], discharg...   \n",
       "15062  [admission, date, :, [, ****-**-**, ], dischar...   \n",
       "1383   [admission, date, :, [, ****-*-**, ], discharg...   \n",
       "17412  [admission, date, :, [, ****-*-*, ], discharge...   \n",
       "\n",
       "                                               Sentences  \n",
       "565    [admission date:  [****-*-**]              dis...  \n",
       "18269  [admission date:  [****-*-**]              dis...  \n",
       "15062  [admission date:  [****-**-**]       discharge...  \n",
       "1383   [admission date:  [****-*-**]              dis...  \n",
       "17412  [admission date:  [****-*-*]              disc...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TotalNotes['Tokens'] = TotalNotes['TEXT'].map(lambda x: word_tokenize(preprocess(x)))\n",
    "TotalNotes['Sentences'] = TotalNotes['TEXT'].map(lambda x: sent_tokenize(preprocess(x)))\n",
    "TotalNotes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Vocabulary Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of times each token appears in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TotalNotes = TotalNotes.reset_index(drop=True)\n",
    "\n",
    "Corpus_Counts = Counter()\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    if TotalNotes.loc[i, 'Test'] != 1: #if document is NOT in test set\n",
    "        tokens = TotalNotes['Tokens'][i]\n",
    "        unigram_counts = Counter(tokens) #count number of unigrams\n",
    "        Corpus_Counts.update(unigram_counts) #update the Counter for each discharge summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict vocabulary to tokens that appear at least 5 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38119\n"
     ]
    }
   ],
   "source": [
    "vocabulary5 = create_vocabulary(Corpus_Counts, 5)\n",
    "print(len(vocabulary5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-Of-Vocabulary Words. One approach is to link each OOV word to its nearest word in the vocabulary, using edit distance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131315\n"
     ]
    }
   ],
   "source": [
    "#Gather a list of OOV words\n",
    "total_tokens = list(np.unique(np.concatenate(TotalNotes['Tokens'])))\n",
    "OOVs5 = [x for x in total_tokens if x not in list(vocabulary5.keys())]\n",
    "print(len(OOVs5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein #install and import Levenshtein package \n",
    "#Link each OOV to its closest vocabulary word\n",
    "OOV_to_Vocab_Dict_5 = {}\n",
    "i = 0\n",
    "for token in OOVs5:\n",
    "    i = i+1\n",
    "    if i%20000==0:\n",
    "        print(i)\n",
    "    edit_dis_list = []\n",
    "    for vocab in vocabulary5.keys():\n",
    "        edit_dis = Levenshtein.distance(token, vocab)\n",
    "        edit_dis_list.append((vocab, edit_dis))\n",
    "    edit_dis_list = sorted(edit_dis_list, key=lambda tup: tup[1])\n",
    "    closest_vocab = edit_dis_list[0][0]\n",
    "    OOV_to_Vocab_Dict_5[token] = closest_vocab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an \"expanded vocabulary\" so that each OOV token is mapped to the same index as its closest vocabulary word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11218 11218\n"
     ]
    }
   ],
   "source": [
    "extended_vocabulary5 = vocabulary5.copy()\n",
    "for OOV, vocab_word in OOV_to_Vocab_Dict_5.items():\n",
    "    extended_vocabulary5[OOV] = extended_vocabulary5[vocab_word]\n",
    "    \n",
    "print(extended_vocabulary5['edmematous'], extended_vocabulary5['edematous'])\n",
    "\n",
    "save_as_pkl(extended_vocabulary5, 'ExtendedVocab5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169434"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extended_vocabulary5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then convert each list of tokens (in the \"Tokens\" column) to a list of index numbers from the vocabulary -- e.g. \"I like pie\" becomes [34, 120, 17]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Regular Vocabulary - 5 tokens\n",
    "HADMID_Text_Dict = {}\n",
    "#for each discharge summary, convert tokens to indexes based on vocabulary\n",
    "for i in range(len(TotalNotes)):\n",
    "    tokens = TotalNotes['Tokens'][i]\n",
    "    indexed_document = token_to_index(tokens, vocabulary5)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_Text_Dict[hadm_id] = indexed_document\n",
    "    \n",
    "#Same as directly above, except for the \"Sentences\" column\n",
    "HADMID_TextSent_Dict = {}\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    sentences = TotalNotes['Sentences'][i]\n",
    "    indexed_document = sentence_to_indexes(sentences, vocabulary5, list_format=False)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_TextSent_Dict[hadm_id] = indexed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vocabulary with only Edit Distance\n",
    "HADMID_Leven_Dict = {}\n",
    "#for each discharge summary, convert tokens to indexes based on vocabulary\n",
    "for i in range(len(TotalNotes)):\n",
    "    tokens = TotalNotes['Tokens'][i]\n",
    "    indexed_document = token_to_index(tokens, extended_vocabulary5)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_Leven_Dict[hadm_id] = indexed_document\n",
    "    \n",
    "#Same as directly above, except for the \"Sentences\" column\n",
    "HADMID_LevenSent_Dict = {}\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    sentences = TotalNotes['Sentences'][i]\n",
    "    indexed_document = sentence_to_indexes(sentences, extended_vocabulary5, list_format=False)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_LevenSent_Dict[hadm_id] = indexed_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding\n",
    "Alternative method for dealing with out-of-vocabulary words. To get a better feel for how the algorithm works, please refer to the notebook titled \"Byte Pair Encoding\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: create vocabulary of \"character blocks\". Some of these blocks will be full unigrams; other blocks will simply be character n-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "#Key parameter is number of merges\n",
    "\n",
    "#5000 merges\n",
    "WeirdCts5000 = convert_tokens_into_frequent_character_combinations(Corpus_Counts, 5000)\n",
    "CharVocabulary5000 = create_vocabulary_of_character_combos(WeirdCts5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "#10000 merges\n",
    "WeirdCts10000 = convert_tokens_into_frequent_character_combinations(WeirdCts5000, 5000, ongoing=True )\n",
    "CharVocabulary10000 = create_vocabulary_of_character_combos(WeirdCts10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "#25000 merges\n",
    "WeirdCts25000 = convert_tokens_into_frequent_character_combinations(WeirdCts10000, 15000, ongoing=True)\n",
    "CharVocabulary25000 = create_vocabulary_of_character_combos(WeirdCts25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, translate items from Character Vocabulary to Unigram Vocabulary (named here as byte vocabulary). For example, if \"photo\" = 352 and \"graphy<\\w>\" = 231, the unigram \"photography\" should be represented as [352, 231] in the Byte vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24757\n",
      "157112\n",
      "10013\n",
      "157112\n",
      "5036\n",
      "157112\n"
     ]
    }
   ],
   "source": [
    "ByteVocabulary25000 = map_unigram_to_character_indices(WeirdCts25000, CharVocabulary25000)\n",
    "ByteVocabulary10000 = map_unigram_to_character_indices(WeirdCts10000, CharVocabulary10000)\n",
    "ByteVocabulary5000 = map_unigram_to_character_indices(WeirdCts5000, CharVocabulary5000)\n",
    "\n",
    "print(len(CharVocabulary25000))\n",
    "print(len(ByteVocabulary25000))\n",
    "print(len(CharVocabulary10000))\n",
    "print(len(ByteVocabulary10000))\n",
    "print(len(CharVocabulary5000))\n",
    "print(len(ByteVocabulary5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'re do / redo</w>': 2,\n",
       " 'fr ame -</w>': 1,\n",
       " 'hemi block</w>': 38,\n",
       " 'e stro gen / pro ge ster one</w>': 2,\n",
       " 'nur s ery</w>': 3,\n",
       " 'thy m om a obese</w>': 1,\n",
       " 'cit rul lin ated</w>': 1,\n",
       " 'k it ch en</w>': 142,\n",
       " 'tak e- home</w>': 1,\n",
       " 'min d ful</w>': 4,\n",
       " 'hyper expan ded</w>': 18,\n",
       " 'im a es</w>': 1,\n",
       " '* bms</w>': 5,\n",
       " 'lar ge - scale</w>': 1,\n",
       " 'carb am z -</w>': 7,\n",
       " 'hypo xi a/ flash</w>': 2,\n",
       " 'hemorrha g e. with</w>': 1,\n",
       " 'microcy ts</w>': 1,\n",
       " 'por c ess</w>': 1,\n",
       " 'wa ff le</w>': 26,\n",
       " 'pre- mental</w>': 1,\n",
       " 'nect ar/ thickened</w>': 2,\n",
       " 'pe t te chi ae</w>': 1,\n",
       " '* sed</w>': 1,\n",
       " 'cit o lo pr am</w>': 1,\n",
       " 'sub species</w>': 1,\n",
       " 'mechan ism</w>': 198,\n",
       " \"ker ni g 's</w>\": 1,\n",
       " 'pir ul ent</w>': 1,\n",
       " 'splen ic/ portal</w>': 1,\n",
       " 'drain ag e/ discharge</w>': 1,\n",
       " 'hypo pne ic</w>': 2,\n",
       " 'van i q a</w>': 1,\n",
       " 'sub se q e un tly</w>': 6,\n",
       " 'anti ver t</w>': 19,\n",
       " 'gua i af ension</w>': 1,\n",
       " 'enter o- vaginal</w>': 1,\n",
       " 'w/ oropharyngeal</w>': 1,\n",
       " 'ag gr ic ul ture</w>': 5,\n",
       " 'drain i g</w>': 1,\n",
       " 'syn chron ously</w>': 5,\n",
       " 'un cross</w>': 2,\n",
       " 'sy mil in</w>': 1,\n",
       " 'det als</w>': 1,\n",
       " 'table</w>': 415,\n",
       " 'inci on</w>': 1,\n",
       " 'r- thoracotomy</w>': 2,\n",
       " 'ultras on ographic</w>': 36,\n",
       " 'bro c coli</w>': 9,\n",
       " 'proce eding</w>': 89,\n",
       " 'mild k</w>': 1,\n",
       " 'de ep / productive</w>': 1,\n",
       " 'ma o*</w>': 1,\n",
       " 'chil der n</w>': 2,\n",
       " 'z eti a/ simvastatin</w>': 1,\n",
       " '- oncology</w>': 1,\n",
       " '*. g</w>': 2,\n",
       " 'af gh an i</w>': 2,\n",
       " 'par i</w>': 1,\n",
       " 'ha z ards</w>': 1,\n",
       " 'pp b</w>': 1,\n",
       " 'pp day</w>': 2,\n",
       " 'il ic it</w>': 18,\n",
       " 'sh rin k ers</w>': 12,\n",
       " 'ment ation/ somnolence</w>': 2,\n",
       " 'lan os ol id</w>': 1,\n",
       " 'inf or tun ately</w>': 1,\n",
       " 're- distention</w>': 1,\n",
       " 'ci v c u</w>': 1,\n",
       " 'pon s/ mid brain</w>': 2,\n",
       " 'acti no bac tor</w>': 1,\n",
       " 'shoul der - p end ul um s</w>': 1,\n",
       " 'k a op ec te</w>': 1,\n",
       " 'weak ness/ light</w>': 2,\n",
       " 'back filling</w>': 2,\n",
       " 'syst ol ic s/ **</w>': 2,\n",
       " 'bnzodzp-neg</w>': 710,\n",
       " 'chang es/ weakness</w>': 1,\n",
       " 'm oun i er -</w>': 7,\n",
       " 's rap nel</w>': 1,\n",
       " 'cv r na</w>': 1,\n",
       " 'x r/ l</w>': 1,\n",
       " 'lit er s/ min/m*</w>': 3,\n",
       " 'treat d</w>': 3,\n",
       " 'throm bu s. patient</w>': 1,\n",
       " 'se pt al/ ap ic al/ in f</w>': 1,\n",
       " 'pro duct s/ flu shes</w>': 1,\n",
       " 'ger d- associated</w>': 1,\n",
       " 'in rem is sion</w>': 1,\n",
       " 'stimulation</w>': 1041,\n",
       " '*l ge</w>': 1,\n",
       " 'arthro gram</w>': 2,\n",
       " '** pack- yrs</w>': 1,\n",
       " 'fr c ature</w>': 1,\n",
       " 'weigh t- sp aring</w>': 1,\n",
       " 'cor - difficult</w>': 1,\n",
       " 'dise ase / as</w>': 1,\n",
       " 'h ome ward</w>': 1,\n",
       " 'sk ate bo ard</w>': 1,\n",
       " 'm bc</w>': 2,\n",
       " '* li t</w>': 3,\n",
       " 'di a per - pan ts</w>': 1,\n",
       " 'levaqu ine</w>': 3,\n",
       " 'w/ rll</w>': 3,\n",
       " 'av ap ro l</w>': 1,\n",
       " 'j ak</w>': 1,\n",
       " 'tak as ub o</w>': 4,\n",
       " 'angio t ecta sia</w>': 1,\n",
       " 'c il lium</w>': 1,\n",
       " 'th ank ful</w>': 2,\n",
       " 'mer ely</w>': 49,\n",
       " 'sh art</w>': 1,\n",
       " 'om ni pag ue</w>': 1,\n",
       " 'mcg/kg/ hr</w>': 12,\n",
       " 'ha ou se</w>': 1,\n",
       " 'super o anterior</w>': 2,\n",
       " 'dis tin qu ish</w>': 1,\n",
       " 'co t ton se ed</w>': 1,\n",
       " '**- y r- old</w>': 4,\n",
       " 'possi b ly ly</w>': 1,\n",
       " '- site</w>': 1,\n",
       " 'comm unit y heavy</w>': 1,\n",
       " 'ban da ge - clean / dr y/ intact</w>': 1,\n",
       " 'exerc ize</w>': 1,\n",
       " 'phy l lo des</w>': 3,\n",
       " '*. rit on o vir</w>': 1,\n",
       " 'ap nea /</w>': 2,\n",
       " 'g ay m ar</w>': 4,\n",
       " 'eng ine er ing/ technical</w>': 1,\n",
       " '- borderline</w>': 5,\n",
       " 'pat ch /**</w>': 2,\n",
       " 'rh om bo encephal om yel itis</w>': 1,\n",
       " 'lithi um/ tylenol</w>': 1,\n",
       " '- asth ma / copd</w>': 4,\n",
       " 'glob ular</w>': 26,\n",
       " 'am ir ican</w>': 1,\n",
       " 'v t = ***cc</w>': 1,\n",
       " 'n s r /</w>': 2,\n",
       " 'con f u sin</w>': 1,\n",
       " 'cp a</w>': 25,\n",
       " 'le si on s/ masses</w>': 2,\n",
       " 'caroti d/ cervical</w>': 1,\n",
       " 'fat ally</w>': 3,\n",
       " 'mar ke ting</w>': 27,\n",
       " 'trimethopri m /</w>': 1,\n",
       " 'scal lo p</w>': 12,\n",
       " 'diag no si ed</w>': 2,\n",
       " 'supra aortic</w>': 3,\n",
       " 'consist ant</w>': 88,\n",
       " '*. hypercholesterolemia</w>': 16,\n",
       " '- transferred</w>': 2,\n",
       " 'ic thy osis</w>': 2,\n",
       " 'a associated</w>': 2,\n",
       " 'cultu re -</w>': 63,\n",
       " 'trans g as tir c</w>': 1,\n",
       " 'fi ve/ four</w>': 1,\n",
       " 'dr s</w>': 302,\n",
       " 'transf e w r red</w>': 1,\n",
       " 'des car gan do</w>': 1,\n",
       " '- xray</w>': 1,\n",
       " 'st res s -m i bi</w>': 15,\n",
       " 'ni ref ex</w>': 1,\n",
       " 'im pri si on ed</w>': 1,\n",
       " 'my sa th en ia</w>': 2,\n",
       " 'meth o tre x ate-*</w>': 3,\n",
       " '**- ** gtt</w>': 1,\n",
       " 'd the</w>': 1,\n",
       " 'pv r/ r v ot</w>': 1,\n",
       " '*- hd</w>': 1,\n",
       " 'whi p sti t ch</w>': 1,\n",
       " 'ab li fy</w>': 1,\n",
       " 'previ os ul y</w>': 7,\n",
       " 'air ation</w>': 9,\n",
       " 'e h ri g</w>': 1,\n",
       " 'un sust ain able</w>': 1,\n",
       " 'entero bacter /</w>': 1,\n",
       " 'leg is lation</w>': 1,\n",
       " 'q** hrs</w>': 110,\n",
       " 'vas ospasm</w>': 391,\n",
       " 'voric on o zole</w>': 6,\n",
       " '* post</w>': 1,\n",
       " '**. tramadol</w>': 1,\n",
       " '/ *l.</w>': 16,\n",
       " 'oh</w>': 27,\n",
       " 'patient we b omr</w>': 11,\n",
       " 't*- flare</w>': 1,\n",
       " 't b**</w>': 1,\n",
       " 's ma / sm v</w>': 1,\n",
       " 'cyst oscop y/ clot</w>': 1,\n",
       " 'ceftaz i p ime</w>': 1,\n",
       " 'bi ul ding</w>': 1,\n",
       " 'n/v/ d</w>': 335,\n",
       " 'dec ub / inter tri gu inous</w>': 1,\n",
       " 'am bu atory</w>': 1,\n",
       " 'the o retic</w>': 1,\n",
       " 'self- pro pel s</w>': 1,\n",
       " 'tab q*h</w>': 1,\n",
       " '- ed</w>': 15,\n",
       " 'diagnosis</w>': 27948,\n",
       " \"' hepatitis</w>\": 2,\n",
       " 'seros an g</w>': 62,\n",
       " 'post episode</w>': 1,\n",
       " 're star t ed . blood</w>': 1,\n",
       " 'canc el / re- schedule</w>': 1,\n",
       " 'ad dom inal</w>': 1,\n",
       " '*.*- * mcg/kg/min</w>': 1,\n",
       " 'mer iting</w>': 2,\n",
       " 'te gm en</w>': 1,\n",
       " 'syrin g om yel ia</w>': 9,\n",
       " 'd ye - load</w>': 3,\n",
       " 'electroly tes / fluid</w>': 1,\n",
       " 'sch wan om ma</w>': 5,\n",
       " 'post- intravenously</w>': 1,\n",
       " 'ab ov em en tioned</w>': 4,\n",
       " 'cor n cer n</w>': 1,\n",
       " 'sob/ cardiac</w>': 1,\n",
       " 'vi ox x</w>': 371,\n",
       " 'f om er ly</w>': 1,\n",
       " 'bl ad der scan</w>': 1,\n",
       " 'assay</w>': 501,\n",
       " 'tender ess</w>': 2,\n",
       " 'leuk ocyto sis/ sepsis</w>': 1,\n",
       " 'non bearing</w>': 1,\n",
       " 'cholest ***</w>': 1,\n",
       " '- lp</w>': 3,\n",
       " 'f m c</w>': 3,\n",
       " '- hyper phosph ot emia</w>': 1,\n",
       " 'bio chem ical</w>': 13,\n",
       " 'cardio vascul r</w>': 1,\n",
       " 'sp ut um n</w>': 1,\n",
       " '- enlarged</w>': 1,\n",
       " 'flu id / stress</w>': 1,\n",
       " 'h b fa g</w>': 1,\n",
       " '*** m h</w>': 1,\n",
       " 'lis thesis</w>': 50,\n",
       " 'un fused</w>': 5,\n",
       " 'bo ths</w>': 1,\n",
       " 'ischem ia/ st un ning</w>': 1,\n",
       " 'vasc u al ar</w>': 3,\n",
       " 'a rem il d ly</w>': 3,\n",
       " 'ex am .</w>': 1,\n",
       " 'wean k ess</w>': 1,\n",
       " '+ wheezing</w>': 4,\n",
       " 'ex cor itation</w>': 1,\n",
       " 'spiron ol act one/ lasix</w>': 1,\n",
       " 'man ang ement</w>': 5,\n",
       " 'ure to gram</w>': 2,\n",
       " 'ap pr pri ately</w>': 1,\n",
       " 'defer re ed</w>': 1,\n",
       " 'fe int</w>': 2,\n",
       " 'ri ma -</w>': 4,\n",
       " 'trache ost omy placement</w>': 1,\n",
       " 'fi ol ling</w>': 1,\n",
       " 'thorac os c cop ic</w>': 2,\n",
       " 'cau ther ization</w>': 2,\n",
       " 'bp d</w>': 9,\n",
       " 'post- par tum</w>': 31,\n",
       " 'n p/ ng</w>': 1,\n",
       " 'g ame</w>': 53,\n",
       " 'p tu</w>': 38,\n",
       " 'hemop ty sis/ blood</w>': 1,\n",
       " 'cal ori es /**</w>': 1,\n",
       " 'wa iting</w>': 328,\n",
       " 'cli v us</w>': 64,\n",
       " 'ci platin</w>': 2,\n",
       " 'gri am cing</w>': 1,\n",
       " 'line- cvp</w>': 1,\n",
       " 'work - up / management</w>': 1,\n",
       " 'melan o tic - colored</w>': 1,\n",
       " 'ir f</w>': 1,\n",
       " 'ear</w>': 1044,\n",
       " 'expos ed/ infected</w>': 2,\n",
       " 'micro d antin</w>': 1,\n",
       " 'prot ec tion/ intub ation/ somnolence</w>': 1,\n",
       " 'per u ct aneous</w>': 1,\n",
       " 'iv - contrast</w>': 2,\n",
       " 'd . **</w>': 5,\n",
       " 'cc * b</w>': 9,\n",
       " 'ct- perfusion</w>': 2,\n",
       " 'e ali er</w>': 1,\n",
       " 'dec lin in ng</w>': 1,\n",
       " 'ash y</w>': 1,\n",
       " 'cau gh / n/v</w>': 1,\n",
       " 'dri i ft</w>': 1,\n",
       " '- nodule</w>': 1,\n",
       " 'flu id / hematoma</w>': 1,\n",
       " 'cal tre x</w>': 1,\n",
       " 'ar v ost atin</w>': 1,\n",
       " '*h</w>': 348,\n",
       " 'cy stectomy</w>': 165,\n",
       " 'extreme</w>': 447,\n",
       " 'benz april</w>': 2,\n",
       " 'ex tin ct</w>': 1,\n",
       " 'swol len / cold</w>': 1,\n",
       " 'diab eto logist</w>': 20,\n",
       " 'suppl im ental</w>': 7,\n",
       " 'p/ r</w>': 1,\n",
       " 'drain ag e/ pus</w>': 1,\n",
       " 'diabet as our ce</w>': 1,\n",
       " 'ar al ast</w>': 7,\n",
       " 'aller gi es/ medication</w>': 1,\n",
       " 'b he a</w>': 1,\n",
       " 'levo / fl g ay l</w>': 1,\n",
       " 'recon st ur ction</w>': 1,\n",
       " 'sm ent ana</w>': 1,\n",
       " 'cover s ant</w>': 3,\n",
       " 'post morbid</w>': 1,\n",
       " 'br any</w>': 1,\n",
       " 'arm</w>': 7242,\n",
       " 'cap ab il ities</w>': 21,\n",
       " 'limit s/ high</w>': 1,\n",
       " 'e state</w>': 95,\n",
       " 'pneumoni a/ sinusitis</w>': 1,\n",
       " 'ren al caps</w>': 2,\n",
       " 'voice</w>': 1860,\n",
       " 'k- tab</w>': 1,\n",
       " 'open - air</w>': 1,\n",
       " 'progres ses</w>': 18,\n",
       " 'seiz ure - had</w>': 1,\n",
       " 'a ic d/ v vi</w>': 2,\n",
       " 'anti- cardio lo pin</w>': 1,\n",
       " 'sep sis/ central</w>': 1,\n",
       " 'ab x / ag res sive</w>': 1,\n",
       " 'minim al/ no</w>': 1,\n",
       " 'mc s- p *-***</w>': 1,\n",
       " 'levo / flag yl / ax i th ro</w>': 1,\n",
       " 'fr uit y- smel ling</w>': 1,\n",
       " '- currently</w>': 15,\n",
       " 'n ch ct- large</w>': 1,\n",
       " 'bp ~ **/**</w>': 1,\n",
       " 'intubated / paralyzed</w>': 1,\n",
       " 'pen tox y fil ine</w>': 3,\n",
       " 'fi bu lectomy</w>': 1,\n",
       " 'h appear d</w>': 1,\n",
       " 't k ae</w>': 2,\n",
       " 'dic / liver</w>': 1,\n",
       " 'v vi - paced</w>': 1,\n",
       " 'to le w r ated</w>': 1,\n",
       " 'transi t on</w>': 1,\n",
       " 'enti er</w>': 1,\n",
       " 'emphy sem a/ copd</w>': 17,\n",
       " 'clon opin</w>': 31,\n",
       " 'st ent / ptca</w>': 3,\n",
       " '/ clin da</w>': 1,\n",
       " 'x */ ma ze</w>': 1,\n",
       " 'ventil ary</w>': 3,\n",
       " 'ch im er ism</w>': 14,\n",
       " 'infl ator</w>': 1,\n",
       " 'vanc/ zos yn / flag yl / flu c</w>': 1,\n",
       " 'phar mary</w>': 1,\n",
       " 'vent- dependence</w>': 6,\n",
       " 'hospital iz ation a</w>': 1,\n",
       " 'screen ed and</w>': 1,\n",
       " 'ser i ously</w>': 31,\n",
       " 'ab nl</w>': 98,\n",
       " 'res et ting</w>': 4,\n",
       " 'sleep / wa ke</w>': 29,\n",
       " 'phen ey le phrine</w>': 1,\n",
       " '*. coagulopathy</w>': 1,\n",
       " 'anis oc ori a/ weakness</w>': 1,\n",
       " 'plac em en tu su al</w>': 1,\n",
       " 'r ate- contro l ling</w>': 2,\n",
       " 'be low- rect ally</w>': 1,\n",
       " 'lin is opril</w>': 4,\n",
       " 'si ck er</w>': 6,\n",
       " 'c fa - ant</w>': 1,\n",
       " '- ar v</w>': 1,\n",
       " 'dem ar c ation</w>': 18,\n",
       " 'hypo per f usi on/ infarcts</w>': 1,\n",
       " \"m un ch aus en 's</w>\": 1,\n",
       " 't ang ential</w>': 87,\n",
       " 'ag -***</w>': 2,\n",
       " 'ket - neg bili -neg</w>': 1,\n",
       " 'ster i ods</w>': 62,\n",
       " 'finasteride</w>': 510,\n",
       " 'chem o regimen</w>': 1,\n",
       " 'mg/ kg/ hour</w>': 1,\n",
       " 'ten se</w>': 308,\n",
       " 'dis as sem b led</w>': 1,\n",
       " 'ba sin</w>': 8,\n",
       " 'sig no foc ant</w>': 1,\n",
       " 'vaso con stric ted</w>': 2,\n",
       " 'depres si on -m s.</w>': 1,\n",
       " 'e he c</w>': 1,\n",
       " 'ab use / alcoholic</w>': 1,\n",
       " 's x s/ diarrhe a/ ur i /</w>': 1,\n",
       " 'pr as u grel</w>': 22,\n",
       " 're- sti t ched</w>': 1,\n",
       " '- depression/ anxiety</w>': 11,\n",
       " 'stat us - a</w>': 1,\n",
       " 'tracheo stenosis</w>': 4,\n",
       " 'ac / *** x **/**</w>': 1,\n",
       " 'admis si o on</w>': 1,\n",
       " 'un treat es</w>': 1,\n",
       " 'minim al -to- no</w>': 1,\n",
       " 'ch r</w>': 16,\n",
       " 'c ent ri ven ular</w>': 6,\n",
       " 'con cur rs</w>': 1,\n",
       " 'in suff ient</w>': 1,\n",
       " 'auto peep ing</w>': 9,\n",
       " 'ec c end tr ic</w>': 1,\n",
       " 'cor rca</w>': 1,\n",
       " 'hypo pne a/ apnea</w>': 2,\n",
       " 'u well</w>': 1,\n",
       " 'approxim ating</w>': 14,\n",
       " 'ste on osis</w>': 1,\n",
       " 'cycl er</w>': 8,\n",
       " 'angiography</w>': 3485,\n",
       " 'pre occ up ied</w>': 5,\n",
       " 'cont un uous</w>': 1,\n",
       " '*+ ppd</w>': 1,\n",
       " 'w rest ling</w>': 2,\n",
       " 'fac ill ty</w>': 1,\n",
       " '~ ** pack</w>': 2,\n",
       " 'hematochez ia/ mel en a/ brbpr</w>': 2,\n",
       " 'ci g/ da y /**</w>': 1,\n",
       " 'anxious</w>': 927,\n",
       " 'leuk em oid</w>': 20,\n",
       " 'p ci x*</w>': 6,\n",
       " 'inf x</w>': 31,\n",
       " 'im atin ab</w>': 1,\n",
       " 'nutri tion - wise</w>': 3,\n",
       " 'distribution</w>': 1044,\n",
       " 'evol v ling</w>': 1,\n",
       " 'ger it ol</w>': 1,\n",
       " 'ac iti v ty</w>': 1,\n",
       " 'l . buttock</w>': 1,\n",
       " 'somn il en c e. head</w>': 1,\n",
       " 'monitoring</w>': 7307,\n",
       " 'th ings</w>': 274,\n",
       " 'g- r</w>': 1,\n",
       " 'd ark - brown</w>': 1,\n",
       " \"** n rbc ' s/ *** wbc 's</w>\": 1,\n",
       " 'toxic it y/ overdose</w>': 1,\n",
       " 'hemat om as / angi omas</w>': 2,\n",
       " 'pro ct oscopy</w>': 2,\n",
       " '* fio*</w>': 2,\n",
       " 'cry os</w>': 3,\n",
       " 'n c ss</w>': 22,\n",
       " 'in on ized</w>': 1,\n",
       " '**. oxyco d one/ acetaminophen</w>': 1,\n",
       " 'se oc n d ary</w>': 4,\n",
       " 'cle a res</w>': 1,\n",
       " 'y si cal</w>': 1,\n",
       " 'p es ad a</w>': 1,\n",
       " 'a/ m</w>': 2,\n",
       " 'weak ness/ </w>': 13,\n",
       " 'embol i/ clot</w>': 1,\n",
       " 'pneumoni a/ septic</w>': 2,\n",
       " '* te as po on</w>': 1,\n",
       " 'or ga q n is ms</w>': 1,\n",
       " 'scat ter red</w>': 8,\n",
       " 'predn i es one</w>': 1,\n",
       " 'end- to- end</w>': 34,\n",
       " 'os c illo p sia</w>': 1,\n",
       " 'dimin ish ment</w>': 5,\n",
       " 'behavi or</w>': 415,\n",
       " 'up tr itr ated</w>': 2,\n",
       " 'lat an ap ro st</w>': 2,\n",
       " 'as sy m</w>': 2,\n",
       " 'bu le mia</w>': 21,\n",
       " 'cle an- margin</w>': 2,\n",
       " '- tempor om an di bular</w>': 3,\n",
       " 'dr unk</w>': 55,\n",
       " 'pri or ities</w>': 2,\n",
       " 'pl ate ua</w>': 2,\n",
       " 'per hi per al</w>': 1,\n",
       " 'inter current</w>': 26,\n",
       " 'neu olog ical</w>': 3,\n",
       " 'pat ch es/ wk</w>': 1,\n",
       " 'con struc tion - office</w>': 1,\n",
       " '- flight</w>': 2,\n",
       " '+ breath</w>': 2,\n",
       " 'trache a- esophageal</w>': 1,\n",
       " '**. resection</w>': 1,\n",
       " 'diuresed</w>': 4673,\n",
       " 'ce fe o ime</w>': 1,\n",
       " 'al on g side</w>': 16,\n",
       " 'ti red - appearing</w>': 25,\n",
       " 'pain/ neuropathy</w>': 7,\n",
       " 'admis si on . he</w>': 1,\n",
       " 'fos re no l</w>': 24,\n",
       " 're tre i val</w>': 2,\n",
       " 'extremity</w>': 13207,\n",
       " 'hemo globul in</w>': 1,\n",
       " 'thalam ic/ mid brain</w>': 1,\n",
       " 'dis comfor ted</w>': 1,\n",
       " 'e bi</w>': 2,\n",
       " 'rheumat olog ic/ pulmon ar y/ cardiac</w>': 1,\n",
       " 'concer to</w>': 15,\n",
       " 'ow n . his</w>': 1,\n",
       " 'ac u vit e</w>': 1,\n",
       " 'for tte</w>': 1,\n",
       " 'cultu re x*</w>': 1,\n",
       " 'cor ren tly</w>': 1,\n",
       " 'anti- pcp</w>': 5,\n",
       " 'ch oke</w>': 10,\n",
       " 'sy h throid</w>': 1,\n",
       " 'sep sis/ endocardi ti s/ pna</w>': 1,\n",
       " 'remo v ed/ ster is</w>': 1,\n",
       " '- hem oper it oneum</w>': 1,\n",
       " 'cyto ker atin s</w>': 4,\n",
       " '. . pt</w>': 2,\n",
       " 'qu i ety</w>': 1,\n",
       " 'nsa id s/ anti- inflammatory</w>': 5,\n",
       " 'pne mo thr ac es</w>': 1,\n",
       " 'u ve iti s/ i ri tis -</w>': 1,\n",
       " 're- discussion</w>': 2,\n",
       " 'tem oral</w>': 2,\n",
       " '** h/ off</w>': 1,\n",
       " 'al th o i u gh</w>': 1,\n",
       " 'pancre t ectomy</w>': 1,\n",
       " 'reg ur gi at tion</w>': 1,\n",
       " 'membran o pro li fer ative</w>': 19,\n",
       " 'n d/ nt</w>': 98,\n",
       " 'down - time</w>': 1,\n",
       " 'quad rig em inal</w>': 39,\n",
       " 'sw a</w>': 6,\n",
       " 'inf at abs</w>': 15,\n",
       " 'tr act / kidney</w>': 1,\n",
       " 'inspir ation</w>': 375,\n",
       " 'dom ains</w>': 3,\n",
       " 'oropharynx</w>': 3703,\n",
       " 'ct x /flagyl</w>': 5,\n",
       " 'ery throm yc ine</w>': 1,\n",
       " 'bi b sil ar</w>': 1,\n",
       " 'colon . there</w>': 1,\n",
       " 'un injected</w>': 16,\n",
       " 'sec tional</w>': 22,\n",
       " 'pup illary</w>': 116,\n",
       " \"' knee</w>\": 1,\n",
       " 'pneumoni a/ meningitis</w>': 1,\n",
       " 'w/ young</w>': 1,\n",
       " 'prof an ities</w>': 4,\n",
       " 'respir ation s/ minute</w>': 2,\n",
       " 'block / complete</w>': 1,\n",
       " 'mas s/ cancer</w>': 1,\n",
       " 'nit r ite</w>': 243,\n",
       " 'p ons</w>': 261,\n",
       " 'frac tu re / mal alignment</w>': 1,\n",
       " 'con ti inu ed</w>': 1,\n",
       " 'sh o ved</w>': 1,\n",
       " 'gre ater s</w>': 1,\n",
       " 'un rec table</w>': 1,\n",
       " '- furosemide</w>': 36,\n",
       " 'co wd en</w>': 3,\n",
       " 'et om i da t</w>': 1,\n",
       " 'to e- to -</w>': 2,\n",
       " 'rec a</w>': 2,\n",
       " 'dev o lo ped</w>': 5,\n",
       " 'i . m . q</w>': 1,\n",
       " 'du pl ic ator</w>': 1,\n",
       " 't te = ef **</w>': 1,\n",
       " 'hi / si</w>': 2,\n",
       " 'pancre ali p ase</w>': 2,\n",
       " 'n/ ap ap</w>': 1,\n",
       " 'prednis n e</w>': 1,\n",
       " 'n sv tach</w>': 1,\n",
       " 'x bx</w>': 1,\n",
       " 'hypertro phy u</w>': 1,\n",
       " 'v a ul at</w>': 1,\n",
       " 'epi ose</w>': 1,\n",
       " 'ser rati a/ klebsiella</w>': 1,\n",
       " 'ac ed</w>': 1,\n",
       " 'pi perc illin- tazobactam</w>': 9,\n",
       " '**. ativan</w>': 5,\n",
       " 'swal low- evaluation</w>': 1,\n",
       " 'mam m</w>': 2,\n",
       " 'pl t s-</w>': 1,\n",
       " 'con f usi on/ aphasic</w>': 1,\n",
       " 'foc used</w>': 179,\n",
       " 'trig l</w>': 1,\n",
       " 'dic / hit</w>': 2,\n",
       " 'cre ain ine</w>': 1,\n",
       " 'chr ons</w>': 4,\n",
       " 'pol i om yel itis</w>': 2,\n",
       " 'pre car i ously</w>': 1,\n",
       " 'ha em at oma</w>': 2,\n",
       " 'press or s/ central</w>': 1,\n",
       " 'adhe si on sen ter ectomy</w>': 1,\n",
       " 'depression/ anxi et y/ agitation</w>': 1,\n",
       " 'sur v el li ence</w>': 8,\n",
       " 'pre hydration</w>': 25,\n",
       " 'pre proced ur ally</w>': 1,\n",
       " 'ov con -**</w>': 1,\n",
       " 'swal low ing/ diet</w>': 1,\n",
       " 'men ig eal</w>': 2,\n",
       " '/ re u ben s</w>': 2,\n",
       " 'n stem i . bronch -</w>': 1,\n",
       " 'portions</w>': 741,\n",
       " 'clin da / cipro</w>': 1,\n",
       " 'colon ial</w>': 203,\n",
       " 'amiod or an e</w>': 1,\n",
       " 'obstruc tions</w>': 76,\n",
       " 'fal se -positive</w>': 5,\n",
       " 'esophag ous</w>': 9,\n",
       " 'ta</w>': 94,\n",
       " 'im plant</w>': 277,\n",
       " 'rehab .</w>': 1,\n",
       " 're bolus</w>': 2,\n",
       " 'sub q.</w>': 1,\n",
       " 'nutri tion/ peripheral</w>': 1,\n",
       " 'dise ase / urology</w>': 1,\n",
       " 'the atr e</w>': 9,\n",
       " 'q f</w>': 2,\n",
       " 'neurolog ic/ psychiatric</w>': 7,\n",
       " '~ *x</w>': 5,\n",
       " 'heter o top ia</w>': 2,\n",
       " 'ca us al</w>': 3,\n",
       " 'w all flex</w>': 1,\n",
       " 'gn ar led</w>': 2,\n",
       " 'hydr az al ize</w>': 1,\n",
       " 'eng ag able</w>': 1,\n",
       " 'fibro id s/ infer tility</w>': 1,\n",
       " 'se q ul e</w>': 1,\n",
       " 'en ter</w>': 128,\n",
       " 'techn ic allo y</w>': 1,\n",
       " 'non infected</w>': 12,\n",
       " '- abg</w>': 6,\n",
       " 'inhal er -*</w>': 1,\n",
       " 'to -</w>': 11,\n",
       " 'syst ol</w>': 6,\n",
       " 'reve ale</w>': 4,\n",
       " 'bum ping</w>': 26,\n",
       " '- avoided</w>': 1,\n",
       " 'diag no sis/ medic ation s/ condition</w>': 1,\n",
       " 'new ly - placed</w>': 2,\n",
       " 'ox ali pati n</w>': 1,\n",
       " 'tem po ary</w>': 1,\n",
       " 'fem /</w>': 21,\n",
       " 'k inesis</w>': 9,\n",
       " 'soc i al y</w>': 1,\n",
       " 'ven ti lar tory</w>': 1,\n",
       " 'pain/ cramp ing/ discomfort</w>': 1,\n",
       " 'lid -no</w>': 1,\n",
       " 'ele ven th</w>': 26,\n",
       " 'in ti iated</w>': 2,\n",
       " 'dou bly</w>': 11,\n",
       " 'descri ption</w>': 337,\n",
       " 'suppor t/ chest</w>': 3,\n",
       " 'jaundic e/ pale</w>': 1,\n",
       " 'ischem ia/ colonic</w>': 1,\n",
       " 'rec u ed</w>': 1,\n",
       " 'sed ations</w>': 4,\n",
       " 'trach om ati s</w>': 9,\n",
       " 'hypo g on a dism</w>': 39,\n",
       " 'o the wise</w>': 7,\n",
       " '**. pravastatin</w>': 1,\n",
       " 'su be qu ently</w>': 4,\n",
       " 'intub ation/ sed ating</w>': 1,\n",
       " 'ch f/ afib</w>': 5,\n",
       " 'an pir ation</w>': 1,\n",
       " 'under estim at ed .</w>': 1,\n",
       " 'fla i res</w>': 1,\n",
       " 'regards</w>': 578,\n",
       " 'di vi sions</w>': 24,\n",
       " 'vanc/ zos yn / levo</w>': 6,\n",
       " '- splenomegaly</w>': 1,\n",
       " 'distress</w>': 13880,\n",
       " 'anti- seizure</w>': 264,\n",
       " 'ir re trac table</w>': 2,\n",
       " 'tre ater s</w>': 19,\n",
       " 're- pro gram m ing</w>': 1,\n",
       " 'sat ge</w>': 3,\n",
       " '. . de od or ant</w>': 1,\n",
       " 'h oun d s field</w>': 4,\n",
       " 'c ve</w>': 2,\n",
       " 'keto ti fen</w>': 6,\n",
       " 'pneumoni a/ methicillin</w>': 1,\n",
       " 'q. *-* h. p.r.n</w>': 9,\n",
       " 'intermit ten t ly / alert</w>': 1,\n",
       " 'pathology</w>': 3049,\n",
       " 'intra v sc ular</w>': 1,\n",
       " 'ultras et</w>': 3,\n",
       " 'diaph r am</w>': 9,\n",
       " 'psy l lu m</w>': 1,\n",
       " 'agit ation /</w>': 4,\n",
       " 'thi gh / inferior</w>': 1,\n",
       " 'pre gn anc y/ deli very</w>': 1,\n",
       " 'op tion care</w>': 1,\n",
       " 'si ri us</w>': 2,\n",
       " 'mos qu it o es</w>': 1,\n",
       " 'run - over</w>': 1,\n",
       " 'shi py ard</w>': 28,\n",
       " '- ruptured</w>': 1,\n",
       " 'prot ec tively</w>': 1,\n",
       " '- bc x -</w>': 1,\n",
       " 'ep os o dic</w>': 1,\n",
       " 'k cal / ** g</w>': 1,\n",
       " 'organis mal</w>': 2,\n",
       " '** f x *cm</w>': 2,\n",
       " 'cit opram</w>': 1,\n",
       " 'necessit ated</w>': 101,\n",
       " 'ther ap</w>': 2,\n",
       " 'ser olog y- final</w>': 2,\n",
       " '- discharged</w>': 6,\n",
       " 're ba il itation</w>': 2,\n",
       " 'co re +</w>': 1,\n",
       " 'bil ate ar l ly</w>': 8,\n",
       " '+ dizziness</w>': 7,\n",
       " 'ty g ac il</w>': 1,\n",
       " 'trans m initis</w>': 2,\n",
       " 'mal ar</w>': 21,\n",
       " 'ther er</w>': 1,\n",
       " 'dres s/ feed</w>': 1,\n",
       " '***/ oxygen</w>': 1,\n",
       " '*. circum ci sion</w>': 1,\n",
       " 'pseu o dach al asia</w>': 1,\n",
       " 'c- r</w>': 1,\n",
       " 'ar an ge</w>': 3,\n",
       " 'hemat olog y- oncology</w>': 34,\n",
       " 'incompletely</w>': 449,\n",
       " 'heart line</w>': 1,\n",
       " 'narro w d</w>': 1,\n",
       " 'ob atin</w>': 2,\n",
       " 'lu e ks</w>': 2,\n",
       " 'h en at emesis</w>': 1,\n",
       " 'ps chi atry</w>': 1,\n",
       " 'post- pneumonia</w>': 2,\n",
       " 'syn cop ised</w>': 1,\n",
       " 'pe dis / *+</w>': 3,\n",
       " 'm la</w>': 1,\n",
       " 'su s se ss fully</w>': 1,\n",
       " 'anti hist amin es</w>': 44,\n",
       " 'up - wise</w>': 1,\n",
       " 'resul tes</w>': 1,\n",
       " 'bp ***-***/ ***-***</w>': 2,\n",
       " 'p cp/ other</w>': 2,\n",
       " 'to- moderate</w>': 8,\n",
       " 'ten d/ nd</w>': 1,\n",
       " 'fevers/ ch ill s/ mala i se / abd</w>': 1,\n",
       " 'ciwa</w>': 1209,\n",
       " 'az act am</w>': 2,\n",
       " 'ren al/ pulmonary</w>': 5,\n",
       " 'tr as hepatic</w>': 1,\n",
       " 'o. u . b.i.d</w>': 3,\n",
       " 'in- la ws</w>': 7,\n",
       " 'sc lap</w>': 2,\n",
       " 'u a/ u c</w>': 1,\n",
       " 'meg ally</w>': 1,\n",
       " 'ev n ts</w>': 1,\n",
       " '- apply</w>': 8,\n",
       " 'levo / vas o</w>': 2,\n",
       " 'cp/ sob/ dizzy</w>': 1,\n",
       " 'i re not ec an / c is platin</w>': 3,\n",
       " 'di pho resis</w>': 5,\n",
       " 'flu tis ac one</w>': 1,\n",
       " 'py ro plasty</w>': 1,\n",
       " 'gro wh t</w>': 1,\n",
       " 'l ge</w>': 4,\n",
       " 'ren ag el / phos lo</w>': 1,\n",
       " 'i . v . b.i.d.</w>': 2,\n",
       " 'fac et ectomy</w>': 16,\n",
       " 'gastro intes tin al/ fluids</w>': 3,\n",
       " 'bo ok - ke ep er</w>': 4,\n",
       " 'bic al ve</w>': 1,\n",
       " 'colle c tion s- post- cr ic oid</w>': 1,\n",
       " '* mg m/ hr</w>': 1,\n",
       " 're star il</w>': 1,\n",
       " 'hos ti pal</w>': 1,\n",
       " 'me tro lotion</w>': 2,\n",
       " 'medic ati on- ar av a</w>': 1,\n",
       " 'mal asi e</w>': 3,\n",
       " 'lima- diag - lad</w>': 1,\n",
       " 'tachy cardi at</w>': 1,\n",
       " 'clo mid</w>': 1,\n",
       " '. cad</w>': 2,\n",
       " 'e. k</w>': 1,\n",
       " 'j- ps</w>': 3,\n",
       " 'broad -</w>': 6,\n",
       " 'eth yc rin ic</w>': 3,\n",
       " \"*** ' s- ***s</w>\": 1,\n",
       " 'aztreon am / levo flo x</w>': 1,\n",
       " 'par od ox us</w>': 3,\n",
       " 'bacitr ac in/ poly my x in</w>': 4,\n",
       " 's acc ad es/ pursu its</w>': 2,\n",
       " 'medic al /</w>': 1,\n",
       " 'k ling</w>': 6,\n",
       " 'gem cit ab ine/ ox ali platin</w>': 3,\n",
       " '* v- cabg</w>': 26,\n",
       " '/ cx</w>': 1,\n",
       " '- episode</w>': 3,\n",
       " 'c ut an s ous</w>': 1,\n",
       " 'cor ac oc lavicular</w>': 2,\n",
       " 're- instructed</w>': 1,\n",
       " 'fin din g s/ w ma</w>': 1,\n",
       " 'dise ase / esophagitis</w>': 1,\n",
       " 'bleed - admitted</w>': 2,\n",
       " 't s-</w>': 1,\n",
       " 'mon t ul ukast</w>': 1,\n",
       " 'ad jun ci ve</w>': 1,\n",
       " 'ile us /</w>': 4,\n",
       " 'ca ***-**</w>': 8,\n",
       " 'fibr il iation</w>': 1,\n",
       " 'ev ery ting</w>': 1,\n",
       " 'to le ated</w>': 1,\n",
       " 'c ra y ons</w>': 1,\n",
       " 'vaso pressor</w>': 181,\n",
       " 'sh ou ting</w>': 23,\n",
       " 'i on/ follow-up</w>': 1,\n",
       " 'hepati ti s/ obstructive</w>': 1,\n",
       " 'ast =**</w>': 4,\n",
       " 'ev er ts</w>': 2,\n",
       " '*. pubic</w>': 1,\n",
       " 'brbpr</w>': 1491,\n",
       " 'drain ag e/ p us/ bleeding</w>': 1,\n",
       " 'tom ain tain</w>': 1,\n",
       " 'chlor oma</w>': 6,\n",
       " 'bed - electr ic</w>': 1,\n",
       " 'thr ac ic</w>': 1,\n",
       " 'plan n ing/ mar king</w>': 1,\n",
       " 'pal e- elderly</w>': 1,\n",
       " 'd j d- neck</w>': 2,\n",
       " 'attem pt/ ideation</w>': 1,\n",
       " 'h bg</w>': 11,\n",
       " 'ster n al- healing</w>': 1,\n",
       " 'auto bmt</w>': 4,\n",
       " 'cramp iness</w>': 2,\n",
       " 'in tak e/ no</w>': 1,\n",
       " 'em o tion al y</w>': 1,\n",
       " 't feeds</w>': 1,\n",
       " 'it ral</w>': 2,\n",
       " 'com it an tes</w>': 1,\n",
       " 'fl ank / rib</w>': 1,\n",
       " 'j- point</w>': 47,\n",
       " 'c*- c */ fusion</w>': 1,\n",
       " 'c- pnd</w>': 2,\n",
       " 'as s si t</w>': 2,\n",
       " 'dil ti alz em</w>': 1,\n",
       " 'te am / contact</w>': 1,\n",
       " 'pul m/ adrenal</w>': 1,\n",
       " 'ic u/ med</w>': 1,\n",
       " 'seiz ures / altered</w>': 1,\n",
       " 'pe tition</w>': 1,\n",
       " 'bol ii</w>': 1,\n",
       " 'disc - fusion</w>': 1,\n",
       " 'extub ati ing</w>': 1,\n",
       " 'bru d z yn sk i</w>': 2,\n",
       " 'met ac ar po car pal</w>': 2,\n",
       " 'excur ion</w>': 1,\n",
       " 'le u loc yto sis</w>': 1,\n",
       " '*. pt</w>': 6,\n",
       " '/ leg</w>': 1,\n",
       " 'bid</w>': 32932,\n",
       " '- peri- op</w>': 1,\n",
       " 'ventil ations</w>': 10,\n",
       " 's acro ile iti s/ epidural</w>': 1,\n",
       " 'gastr in- test</w>': 1,\n",
       " '- pro ch loper azine</w>': 1,\n",
       " 'reco il</w>': 8,\n",
       " 'ox id ative</w>': 1,\n",
       " '** md</w>': 4,\n",
       " 'bri ak</w>': 1,\n",
       " 'mucou s- man y **/**/**</w>': 1,\n",
       " 'kayex al et e</w>': 1,\n",
       " 'cp ap / pressure</w>': 1,\n",
       " 'shi g ella</w>': 134,\n",
       " 'mid brain</w>': 90,\n",
       " 'atyp ically</w>': 5,\n",
       " '*. concern</w>': 1,\n",
       " 'a a/ na</w>': 2,\n",
       " 'g -m g/**</w>': 1,\n",
       " 'yeast / enteroc occ us/ mrsa</w>': 1,\n",
       " 'tachy - arrhythmia</w>': 7,\n",
       " 'neph rect om y/ partial</w>': 1,\n",
       " 'car ni val</w>': 5,\n",
       " 'fevers/ abd</w>': 1,\n",
       " 'p cta</w>': 23,\n",
       " '* p pi + nr ti </w>': 1,\n",
       " 'hy p no trem ia</w>': 1,\n",
       " 'centr a</w>': 3,\n",
       " 'un her al ded</w>': 2,\n",
       " 'gt tp</w>': 8,\n",
       " 'app ra ren t</w>': 1,\n",
       " 'expl i atives</w>': 1,\n",
       " 'li f ecare</w>': 27,\n",
       " '- ciprofloxacin</w>': 10,\n",
       " 'tissu e/ pathology</w>': 1,\n",
       " 'jo int / muscle</w>': 4,\n",
       " 'ventil ator - induced</w>': 1,\n",
       " 'gentle e</w>': 1,\n",
       " 'units/ hr</w>': 107,\n",
       " '**. serevent</w>': 1,\n",
       " 'hc g-</w>': 57,\n",
       " 'pre qu ir ing</w>': 1,\n",
       " 'hap / bronchitis</w>': 1,\n",
       " 'func tion/ echo</w>': 1,\n",
       " 'w/ jejunostomy</w>': 1,\n",
       " 'e kg/ s xs</w>': 1,\n",
       " 'tran su dation</w>': 1,\n",
       " 'th in- section</w>': 4,\n",
       " 'ator v statin</w>': 4,\n",
       " 'di phasic</w>': 1,\n",
       " 'name / number</w>': 1,\n",
       " \"k a posi 's</w>\": 1,\n",
       " 'pro jection</w>': 121,\n",
       " 'h ell p</w>': 9,\n",
       " 'ro hy p no l</w>': 2,\n",
       " '*- *- **-***</w>': 1,\n",
       " 'chlor thiazide</w>': 11,\n",
       " 'syn dro me / cardiac</w>': 1,\n",
       " 'ra p</w>': 48,\n",
       " 'prot ec tin</w>': 1,\n",
       " 'k lar on</w>': 1,\n",
       " 'bridge</w>': 703,\n",
       " 'dis loc ations</w>': 130,\n",
       " 'n/ *</w>': 1,\n",
       " 'ph t</w>': 33,\n",
       " 'frac tu re / sternal</w>': 1,\n",
       " '**. multiple</w>': 2,\n",
       " 'humal o g/ ssi</w>': 1,\n",
       " 'cou s ne ling</w>': 1,\n",
       " 'os et om yel itis</w>': 3,\n",
       " 'we ther</w>': 6,\n",
       " '***- low</w>': 2,\n",
       " 'e thic as</w>': 1,\n",
       " 'eryth em at ou s/ ecchy motic</w>': 1,\n",
       " 'nit ep ri de</w>': 1,\n",
       " 'radiograph</w>': 1688,\n",
       " 'pi p es/ day</w>': 1,\n",
       " 'asth ma / bronchiectasis</w>': 5,\n",
       " 'asked</w>': 1240,\n",
       " 'fever/ facial</w>': 1,\n",
       " 'bl ac k / brown</w>': 2,\n",
       " 'hypo v ele mic</w>': 2,\n",
       " 'tun n y</w>': 1,\n",
       " '**- **- *l</w>': 1,\n",
       " 'diagnostic</w>': 1756,\n",
       " '- postoperative</w>': 1,\n",
       " 'bili ri b in</w>': 1,\n",
       " 'eto h/ cocain e/ iv</w>': 1,\n",
       " 'pal mar is</w>': 2,\n",
       " 'ap tro pine</w>': 1,\n",
       " '*** sm</w>': 1,\n",
       " 'x- fi x</w>': 2,\n",
       " 'lev el . on</w>': 1,\n",
       " 'levo / g ent</w>': 1,\n",
       " 're dis t</w>': 1,\n",
       " 'stem i/ cad / chf</w>': 1,\n",
       " 'prot ec ti ve - effect</w>': 1,\n",
       " 'f up</w>': 5,\n",
       " 'structur al- free</w>': 2,\n",
       " 'per l</w>': 51,\n",
       " '**. wellbutrin</w>': 2,\n",
       " 'rehab / other</w>': 1,\n",
       " 'pu d ding</w>': 77,\n",
       " 'surg ery / anticoagulation</w>': 1,\n",
       " 'li th</w>': 1,\n",
       " 'att ac ker</w>': 3,\n",
       " '** m q</w>': 1,\n",
       " 'gr un tin g/ g as ping</w>': 1,\n",
       " 'm vi / vit</w>': 2,\n",
       " 'hep a itis</w>': 1,\n",
       " 'open ing ..</w>': 1,\n",
       " 's pot ted</w>': 15,\n",
       " 'sternocleidom ast</w>': 1,\n",
       " 'tra chi o bronch im al acia</w>': 1,\n",
       " 'k ul and</w>': 1,\n",
       " 'sh d</w>': 2,\n",
       " 'angio ec t atic</w>': 1,\n",
       " 'me ticul ou os</w>': 1,\n",
       " 'arth ri tis -</w>': 6,\n",
       " 'pe er la</w>': 37,\n",
       " 'ele der ly</w>': 10,\n",
       " 'l le -*+</w>': 1,\n",
       " '* insulin</w>': 1,\n",
       " 'instruc it on</w>': 1,\n",
       " 'd ine ph ro pathy</w>': 6,\n",
       " 'endoc r ine/ hematology</w>': 1,\n",
       " 'contro l le d/ resolved</w>': 1,\n",
       " 'levaqu in/ zosyn</w>': 1,\n",
       " 'tric lo s an</w>': 1,\n",
       " 'diaph retic</w>': 2,\n",
       " 'characterize</w>': 504,\n",
       " 'anti thy mo</w>': 1,\n",
       " 'trans der</w>': 1,\n",
       " '- aortic</w>': 25,\n",
       " 'u til</w>': 1,\n",
       " 'en ab le x</w>': 17,\n",
       " '*. peptic</w>': 1,\n",
       " 'ret es ting</w>': 4,\n",
       " 'zos yn / azithromycin</w>': 3,\n",
       " 'reas su res</w>': 1,\n",
       " 'b lu e/ green</w>': 1,\n",
       " 'st ain able</w>': 33,\n",
       " 'seiz ure - type</w>': 3,\n",
       " 'g on oc occal</w>': 8,\n",
       " 'hypok al em ia/ f t t</w>': 1,\n",
       " 'pa in- disc</w>': 8,\n",
       " 'addi ti no al</w>': 1,\n",
       " '- quit</w>': 19,\n",
       " 'ec ch my osis</w>': 1,\n",
       " '*/ **/ **- */**</w>': 1,\n",
       " 'yel lo wi sh / gre en ish</w>': 1,\n",
       " 'm inter al</w>': 1,\n",
       " 'ce tral</w>': 3,\n",
       " 'asbe st os - exposure</w>': 1,\n",
       " 'incre ment</w>': 11,\n",
       " 'rise</w>': 1401,\n",
       " 'sp ry</w>': 47,\n",
       " ...}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WeirdCts10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code for saving the vocabularies\n",
    "save_as_pkl(ByteVocabulary25000, 'Byte25000Vocabulary')\n",
    "save_as_pkl(CharVocabulary25000, 'Char25000Vocabulary')\n",
    "\n",
    "save_as_pkl(ByteVocabulary10000, 'Byte10000Vocabulary')\n",
    "save_as_pkl(CharVocabulary10000, 'Char10000Vocabulary')\n",
    "\n",
    "save_as_pkl(ByteVocabulary5000, 'Byte5000Vocabulary')\n",
    "save_as_pkl(CharVocabulary5000, 'Char5000Vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15692 141420\n",
      "5796 151316\n",
      "2666 154446\n"
     ]
    }
   ],
   "source": [
    "#Count number of unigrams in ByteVocabulary25000 represented by one character block\n",
    "multiples = 0\n",
    "singles = 0\n",
    "for k, v in ByteVocabulary25000.items():\n",
    "    if len(v) == 1:\n",
    "        singles = singles+1\n",
    "    else:\n",
    "        multiples = multiples+1\n",
    "print(singles, multiples)\n",
    "\n",
    "#Count number of unigrams in ByteVocabulary10000 represented by one character block\n",
    "multiples = 0\n",
    "singles = 0\n",
    "for k, v in ByteVocabulary10000.items():\n",
    "    if len(v) == 1:\n",
    "        singles = singles+1\n",
    "    else:\n",
    "        multiples = multiples+1\n",
    "print(singles, multiples)\n",
    "\n",
    "#Count number of unigrams in ByteVocabulary5000 represented by one character block\n",
    "multiples = 0\n",
    "singles = 0\n",
    "for k, v in ByteVocabulary5000.items():\n",
    "    if len(v) == 1:\n",
    "        singles = singles+1\n",
    "    else:\n",
    "        multiples = multiples+1\n",
    "print(singles, multiples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with words that only appear in the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12322"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester0 = TotalNotes[TotalNotes['Test']==1]\n",
    "tester = tester0.reset_index(drop=True)\n",
    "test_tokens = list(np.unique(np.concatenate(tester['Tokens'])))\n",
    "unique_test_tokens = [x for x in test_tokens if x not in list(ByteVocabulary5000.keys())]\n",
    "len(unique_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_ngrams(CharVocab, test_tokens):\n",
    "    '''\n",
    "    This function attempts to map unknown test set words to combinations of character blocks.\n",
    "    '''\n",
    "    non_enders = [bloc for bloc in list(CharVocab.keys()) if '</w>' not in bloc]\n",
    "    enders = [bloc for bloc in list(CharVocab.keys()) if '</w>' in bloc]\n",
    "\n",
    "    i = 0\n",
    "    Ngrams = {}\n",
    "    for u in test_tokens:\n",
    "        \n",
    "        #find best combination of character blocks\n",
    "        u = u + '</w>'\n",
    "        Found = False\n",
    "        first_candidates = [f for f in non_enders if (u[:len(f)] == f)]\n",
    "        first_candidates.sort(key=len, reverse=True)\n",
    "    \n",
    "        for f in first_candidates:\n",
    "            remaining = u[len(f):]        \n",
    "        \n",
    "            #bigrams\n",
    "            for e in enders:\n",
    "                if e == remaining:\n",
    "                    Ngrams[u[:-4]] = [CharVocab[f], CharVocab[e]]\n",
    "                    Found = True\n",
    "                    break\n",
    "            if Found == True:\n",
    "                break\n",
    "            \n",
    "            #trigrams\n",
    "            second_candidates = [s for s in non_enders if (remaining[:len(s)] == s)]\n",
    "            second_candidates.sort(key=len, reverse=True)\n",
    "            for s in second_candidates:\n",
    "                remaining2 = remaining[len(s):]\n",
    "                for e in enders:\n",
    "                    if e == remaining2:\n",
    "                        Ngrams[u[:-4]] = [CharVocab[f],CharVocab[s],CharVocab[e]]\n",
    "                        Found = True\n",
    "                        break\n",
    "                if Found == True:\n",
    "                    break\n",
    "        \n",
    "                #4-grams\n",
    "                third_candidates = [t for t in non_enders if (remaining2[:len(t)] == t)]\n",
    "                third_candidates.sort(key=len, reverse=True)\n",
    "\n",
    "                for t in third_candidates:\n",
    "                    last = remaining2[len(t):]\n",
    "                    for e in enders:\n",
    "                        if e == last:\n",
    "                            Ngrams[u[:-4]] = [CharVocab[f], CharVocab[s], \n",
    "                                       CharVocab[t], CharVocab[e]]\n",
    "                            Found = True\n",
    "                            break\n",
    "        \n",
    "            if Found == True:\n",
    "                break\n",
    "                \n",
    "    return Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NGrams5K = find_ngrams(CharVocabulary5000, unique_test_tokens)\n",
    "NGrams10K = find_ngrams(CharVocabulary10000, unique_test_tokens)\n",
    "NGrams25K = find_ngrams(CharVocabulary25000, unique_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ByteVocabulary5000_new = dict(ByteVocabulary5000, **NGrams5K)\n",
    "ByteVocabulary10000_new = dict(ByteVocabulary10000, **NGrams10K)\n",
    "ByteVocabulary25000_new = dict(ByteVocabulary25000, **NGrams25K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15692 153017\n",
      "5796 161409\n",
      "2666 162782\n"
     ]
    }
   ],
   "source": [
    "#Count number of unigrams in ByteVocabulary25000 represented by one character block\n",
    "multiples = 0\n",
    "singles = 0\n",
    "for k, v in ByteVocabulary25000_new.items():\n",
    "    if len(v) == 1:\n",
    "        singles = singles+1\n",
    "    else:\n",
    "        multiples = multiples+1\n",
    "print(singles, multiples)\n",
    "\n",
    "#Count number of unigrams in ByteVocabulary10000 represented by one character block\n",
    "multiples = 0\n",
    "singles = 0\n",
    "for k, v in ByteVocabulary10000_new.items():\n",
    "    if len(v) == 1:\n",
    "        singles = singles+1\n",
    "    else:\n",
    "        multiples = multiples+1\n",
    "print(singles, multiples)\n",
    "\n",
    "#Count number of unigrams in ByteVocabulary5000 represented by one character block\n",
    "multiples = 0\n",
    "singles = 0\n",
    "for k, v in ByteVocabulary5000_new.items():\n",
    "    if len(v) == 1:\n",
    "        singles = singles+1\n",
    "    else:\n",
    "        multiples = multiples+1\n",
    "print(singles, multiples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occu pati onal</w>\n",
      "stasis</w>\n"
     ]
    }
   ],
   "source": [
    "Y = WeirdCts5000.copy()\n",
    "for word, value in Y.items():\n",
    "    if value == 1484:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Corpus_Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15551\n"
     ]
    }
   ],
   "source": [
    "Vocabulary = []\n",
    "for key in Corpus_Counts:\n",
    "    value = Corpus_Counts[key]\n",
    "    if value>=34:\n",
    "        Vocabulary.append(key)\n",
    "print(len(Vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157112"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Corpus_Counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert summaries to indexed numbers from Byte Vocabulary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Byte with 25000 merges\n",
    "#Tokens are words\n",
    "HADMID_Byte25000_Dict = {}\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    tokens = TotalNotes['Tokens'][i]\n",
    "    indexed_document = token_to_index(tokens, ByteVocabulary25000_new, list_format=True)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_Byte25000_Dict[hadm_id] = indexed_document\n",
    "    \n",
    "#For Sentences\n",
    "HADMID_Byte25000Sent_Dict = {}\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    sentences = TotalNotes['Sentences'][i]\n",
    "    indexed_document = sentence_to_indexes(sentences, ByteVocabulary25000_new, list_format=True)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_Byte25000Sent_Dict[hadm_id] = indexed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Byte with 10000 merges\n",
    "#Tokens are words\n",
    "HADMID_Byte10000_Dict = {}\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    tokens = TotalNotes['Tokens'][i]\n",
    "    indexed_document = token_to_index(tokens, ByteVocabulary10000_new, list_format=True)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_Byte10000_Dict[hadm_id] = indexed_document\n",
    "    \n",
    "#For Sentences\n",
    "HADMID_Byte10000Sent_Dict = {}\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    sentences = TotalNotes['Sentences'][i]\n",
    "    indexed_document = sentence_to_indexes(sentences, ByteVocabulary10000_new, list_format=True)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_Byte10000Sent_Dict[hadm_id] = indexed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Byte with 5000 merges\n",
    "#Tokens are words\n",
    "HADMID_Byte5000_Dict = {}\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    tokens = TotalNotes['Tokens'][i]\n",
    "    indexed_document = token_to_index(tokens, ByteVocabulary5000_new, list_format=True)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_Byte5000_Dict[hadm_id] = indexed_document\n",
    "    \n",
    "#For Sentences\n",
    "HADMID_Byte5000Sent_Dict = {}\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    sentences = TotalNotes['Sentences'][i]\n",
    "    indexed_document = sentence_to_indexes(sentences, ByteVocabulary5000_new, list_format=True)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_Byte5000Sent_Dict[hadm_id] = indexed_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Experiment: Hybrid Approach\n",
    "Apply edit distance only when edit distance is less than 2. Otherwise, to deal with the remaining OOVs, use Byte-Pair Encoding. Byte-Pair encoding useful for rare words that are super long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein #install and import Levenshtein package \n",
    "#Link an OOV to its closest vocabulary word only if edit distance is 2 or below. \n",
    "\n",
    "OOV_to_Vocab_Dict_SmallDiff5 = {}\n",
    "i = 0\n",
    "for token in OOVs5:\n",
    "    i = i+1\n",
    "    if i%20000==0:\n",
    "        print(i)\n",
    "    edit_dis_list = []\n",
    "    for vocab in vocabulary5.keys():\n",
    "        edit_dis = Levenshtein.distance(token, vocab)\n",
    "        edit_dis_list.append((vocab, edit_dis))\n",
    "    edit_dis_list = sorted(edit_dis_list, key=lambda tup: tup[1])\n",
    "    closest_vocab = edit_dis_list[0][0]\n",
    "    closest_dist = edit_dis_list[0][1]\n",
    "    if closest_dist < 2:\n",
    "        OOV_to_Vocab_Dict_SmallDiff5[token] = closest_vocab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change text using this new OOV_to_Vocab_Dict_SmallDiff5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_words_in_text(Tokens_List, OOV_to_Vocab_Dict_SmallDiff):\n",
    "    '''This function replaces OOV words that are close enough to a vocab word to be mapped\n",
    "    to that vocab word. So [I, love, zfasefvas, New, Yerk] should become \n",
    "    [I, love, zfasefvas, New, York]\n",
    "    \n",
    "    @Tokens_List is a list of unigram tokens from a particular document.\n",
    "    '''\n",
    "    for i in range(len(Tokens_List)):\n",
    "        if Tokens_List[i] in OOV_to_Vocab_Dict_SmallDiff.keys():\n",
    "            Tokens_List[i] = OOV_to_Vocab_Dict_SmallDiff[Tokens_List[i]]\n",
    "    return Tokens_List\n",
    "\n",
    "def convert_words_in_text_sentence(Sentence_List, OOV_to_Vocab_Dict_SmallDiff):\n",
    "    '''This function is similar to the one above, but is specifically for a sequence of sentences.\n",
    "    @Sentence_List is a list of sentences from a particular document.\n",
    "    '''\n",
    "    for se in range(len(Sentence_List)):\n",
    "        tokenized_sentence = word_tokenize(Sentence_List[se])\n",
    "        for i in range(len(tokenized_sentence)):\n",
    "            if tokenized_sentence[i] in OOV_to_Vocab_Dict_SmallDiff.keys():\n",
    "                tokenized_sentence[i] = OOV_to_Vocab_Dict_SmallDiff[tokenized_sentence[i]]\n",
    "        Sentence_List[se] = ' '.join(tokenized_sentence)\n",
    "    return Sentence_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is hard .', 'We are in New York .']\n",
      "This is hard .\n"
     ]
    }
   ],
   "source": [
    "sentences = ['This ias hard.', 'We are in New Yerk.']\n",
    "oovt = {'ias': 'is', 'Yerk':'York'}\n",
    "print(convert_words_in_text_sentence(sentences, oovt))\n",
    "print(convert_words_in_text(sentences[0], oovt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TotalNotes['Tokens-Hybrid'] = TotalNotes['Tokens'].map \\\n",
    "(lambda x: convert_words_in_text(x, OOV_to_Vocab_Dict_SmallDiff5))\n",
    "\n",
    "TotalNotes['Sentences-Hybrid'] = TotalNotes['Sentences'].map \\\n",
    "(lambda x: convert_words_in_text_sentence(x, OOV_to_Vocab_Dict_SmallDiff5))\n",
    "\n",
    "Corpus_Counts_Hybrid = Counter()\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    if TotalNotes.loc[i, 'Test'] != 1: #if document is NOT in test set\n",
    "        tokens = TotalNotes['Tokens-Hybrid'][i]\n",
    "        unigram_counts = Counter(tokens) #count number of unigrams\n",
    "        Corpus_Counts_Hybrid.update(unigram_counts) #update the Counter for each discharge summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157112 104968\n"
     ]
    }
   ],
   "source": [
    "print(len(Corpus_Counts), len(Corpus_Counts_Hybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131315 57201\n"
     ]
    }
   ],
   "source": [
    "#number of OOV words corrected by one edit operation\n",
    "print(len(OOVs5), len(OOV_to_Vocab_Dict_SmallDiff5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with words that only appear in the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7265"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester0 = TotalNotes[TotalNotes['Test']==1]\n",
    "tester = tester0.reset_index(drop=True)\n",
    "test_tokens_hybrid = list(np.unique(np.concatenate(tester['Tokens-Hybrid'])))\n",
    "unique_test_tokens_hybrid = [x for x in test_tokens_hybrid if x not in list(Corpus_Counts_Hybrid.keys())]\n",
    "len(unique_test_tokens_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12322"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now apply byte-pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "#10000 merges\n",
    "WeirdCts10000_Hybrid = convert_tokens_into_frequent_character_combinations(Corpus_Counts_Hybrid, 10000)\n",
    "HybCharVocabulary10000 = create_vocabulary_of_character_combos(WeirdCts10000_Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104968\n",
      "9977\n"
     ]
    }
   ],
   "source": [
    "HybridVocabulary10000 = map_unigram_to_character_indices(WeirdCts10000_Hybrid, HybCharVocabulary10000)\n",
    "print(len(HybridVocabulary10000))\n",
    "print(len(HybCharVocabulary10000))\n",
    "\n",
    "save_as_pkl(HybridVocabulary10000, 'HybridVocabulary10000')\n",
    "save_as_pkl(HybCharVocabulary10000, 'HybCharVocabulary10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NGrams10K_Hybrid = find_ngrams(HybCharVocabulary10000, unique_test_tokens_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5349\n",
      "0.7362697866483139\n"
     ]
    }
   ],
   "source": [
    "print(len(NGrams10K_Hybrid))\n",
    "print(len(NGrams10K_Hybrid)/len(unique_test_tokens_hybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HybridVocabulary10000_new = dict(HybridVocabulary10000, **NGrams10K_Hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5661 104656\n"
     ]
    }
   ],
   "source": [
    "#Count number of tokens in HybridVocabulary10000 represented by one character block\n",
    "multiples = 0\n",
    "singles = 0\n",
    "for k, v in HybridVocabulary10000_new.items():\n",
    "    if len(v) == 1:\n",
    "        singles = singles+1\n",
    "    else:\n",
    "        multiples = multiples+1\n",
    "print(singles, multiples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert summaries (in \"Hybrid\" columns) to indexed numbers from Hybrid Vocabulary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hybrid with 10000 merges\n",
    "#Tokens are words\n",
    "HADMID_Hybrid10000_Dict = {}\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    tokens = TotalNotes['Tokens-Hybrid'][i]\n",
    "    indexed_document = token_to_index(tokens, HybridVocabulary10000_new, list_format=True)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_Hybrid10000_Dict[hadm_id] = indexed_document\n",
    "    \n",
    "#For Sentences\n",
    "HADMID_Hybrid10000Sent_Dict = {}\n",
    "for i in range(len(TotalNotes)): #for each discharge summary\n",
    "    sentences = TotalNotes['Sentences-Hybrid'][i]\n",
    "    indexed_document = sentence_to_indexes(sentences, HybridVocabulary10000_new, list_format=True)\n",
    "    hadm_id = TotalNotes.loc[i, 'HADM_ID']\n",
    "    HADMID_Hybrid10000Sent_Dict[hadm_id] = indexed_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrames so that:\n",
    "- Each row is a HADM_ID; \n",
    "- Columns for each row: the SubjectID, Summary Text in fixed vocabulary (token and sentence form), Summary Text using byte pair encoding (token and sentence form), and ICD-9 Codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in Final DF for MIMIC II: 29683 \n",
      "Rows in MIMIC II Training Set: 20778 \n",
      "Rows in MIMIC II Validation Set: 6075 \n",
      "Rows in MIMIC II Test Set: 2830\n"
     ]
    }
   ],
   "source": [
    "#Codes\n",
    "HADMID_Code_DF = pd.DataFrame(pd.Series(HADMID_Code_Dict)).\\\n",
    "                reset_index().rename(columns={0: 'ICD9_Codes','index': 'HADM_ID'})\n",
    "\n",
    "def create_separate_df(token_dict, sent_dict, IDs, codes, version):\n",
    "    '''This function creates a separare dataframe for each of the 5 preprocessing techniques we will try:\n",
    "    1) fixed vocab with padding; 2) use edit distance for OOV words; 3) Byte pair encoding with \n",
    "    30K merges; 4) Byte pair encoding with 50K merges; and 5) hybrid approach that combines edit distance\n",
    "    and byte pair encoding. \n",
    "    \n",
    "    For each processing technique, we'll generate a dataframe where each row corresponds\n",
    "    to a discharge summary, with columns for HADM_ID, SUBJ_ID, Summary Text tokenized by word/character \n",
    "    block, Summary Text tokenized by sentence, and the ICD codes.\n",
    "    \n",
    "    @token_dict is the dictionary of summaries tokenized by word\n",
    "    @sent_dict is the dictionary of summaries tokenized by sentence\n",
    "    @IDs is a df indicating if HADM_ID corresponds to train or test set\n",
    "    @codes is a df listing the ICD9 codes for each HADM_ID\n",
    "    @version can be 'PlainVocab', 'Leven', 'Byte50K', 'Byte25K', or 'Hybrid20K'\n",
    "    '''\n",
    "\n",
    "    token_df = pd.DataFrame(pd.Series(token_dict)).\\\n",
    "                 reset_index().rename(columns={'index': 'HADM_ID', 0:'Tokens-'+version})\n",
    "    sent_df = pd.DataFrame(pd.Series(sent_dict)).\\\n",
    "                 reset_index().rename(columns={'index': 'HADM_ID', 0:'Sentences-'+version})\n",
    "    final_df = pd.merge(token_df, sent_df, on='HADM_ID')  \n",
    "    final_df = pd.merge(IDs, final_df, on='HADM_ID').sort_values(by=['Test','HADM_ID']).reset_index(drop=True)\n",
    "    \n",
    "    final_df = pd.merge(final_df, codes, on='HADM_ID')\n",
    "    final_df['Length Tokens'] = final_df['Tokens-'+version].map(lambda x: len(x))\n",
    "    final_df['Unique Tokens'] = final_df['Tokens-'+version].map(lambda x: len(np.unique(x)))\n",
    "    final_df['ICD9_Codes_str'] = final_df['ICD9_Codes'].map(lambda x: ' '.join(x))\n",
    "    final_df['Length Sentences'] = final_df['Sentences-'+version].map(lambda x: len(x))\n",
    "\n",
    "\n",
    "    \n",
    "    #Create Validation Set\n",
    "    final_df = final_df.sort_values(by=['Test','HADM_ID']).reset_index(drop=True)\n",
    "    NumTrain = round(0.7*len(final_df))\n",
    "    final_df['Test'] = np.where((final_df.index >= NumTrain)&(final_df['Test']==0), 0.5, final_df['Test'])\n",
    "    return final_df\n",
    "\n",
    "\n",
    "Full_PlainVocab = create_separate_df(HADMID_Text_Dict, HADMID_TextSent_Dict, IDs, HADMID_Code_DF,\n",
    "                                      'PlainVocab')\n",
    "\n",
    "Full_Leven = create_separate_df(HADMID_Leven_Dict, HADMID_LevenSent_Dict, IDs, HADMID_Code_DF,\n",
    "                                      'Leven')\n",
    "\n",
    "Full_Byte5K = create_separate_df(HADMID_Byte5000_Dict, HADMID_Byte5000Sent_Dict, \n",
    "                                    IDs, HADMID_Code_DF, 'Byte5K')\n",
    "\n",
    "Full_Byte10K = create_separate_df(HADMID_Byte10000_Dict, HADMID_Byte10000Sent_Dict, \n",
    "                                    IDs, HADMID_Code_DF, 'Byte10K')\n",
    "\n",
    "Full_Byte25K = create_separate_df(HADMID_Byte25000_Dict, HADMID_Byte25000Sent_Dict, \n",
    "                                    IDs, HADMID_Code_DF, 'Byte25K')\n",
    "\n",
    "Full_Hybrid10K = create_separate_df(HADMID_Hybrid10000_Dict, HADMID_Hybrid10000Sent_Dict, \n",
    "                                   IDs, HADMID_Code_DF, 'Hybrid10K')\n",
    "\n",
    "\n",
    "\n",
    "print('Rows in Final DF for MIMIC II: '+str(len(Full_Leven)),\n",
    "     '\\nRows in MIMIC II Training Set: '+str(len(Full_Leven[Full_Leven['Test']==0])),\n",
    "    '\\nRows in MIMIC II Validation Set: '+str(len(Full_Leven[Full_Leven['Test']==0.5])),\n",
    "     '\\nRows in MIMIC II Test Set: '+str(len(Full_Leven[Full_Leven['Test']==1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018.591887612438\n",
      "2303.1586429943063\n",
      "2123.986221069299\n",
      "2045.2408786173905\n",
      "2121.137924064279\n",
      "147.50143179597748\n"
     ]
    }
   ],
   "source": [
    "print(Full_Leven['Length Tokens'].mean())\n",
    "print(Full_Byte5K['Length Tokens'].mean())\n",
    "print(Full_Byte10K['Length Tokens'].mean())\n",
    "print(Full_Byte25K['Length Tokens'].mean())\n",
    "print(Full_Hybrid10K['Length Tokens'].mean())\n",
    "\n",
    "print(Full_Byte5K['Length Sentences'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018.591887612438\n",
      "6.7230064346595695\n"
     ]
    }
   ],
   "source": [
    "def count_unknowns(token_doc):\n",
    "    '''Count the number of unknown OOV tokens for a particular summary'''\n",
    "    return len([x for x in token_doc if x==1])\n",
    "    \n",
    "Full_PlainVocab['UNKs'] = Full_PlainVocab['Tokens-PlainVocab'].map(lambda x: count_unknowns(x))\n",
    "print(Full_PlainVocab['Length Tokens'].mean())\n",
    "print(Full_PlainVocab['UNKs'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build subset of TotalChart for testing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sub_df(full_chart):\n",
    "    '''This function creates a very small df, for debugging purposes\n",
    "    350 training observations, 100 validation, 50 test.\n",
    "    '''\n",
    "    fc = full_chart[full_chart['Length Sentences']<=120]\n",
    "    SubTrain = fc[fc['Test']==0].head(6300)\n",
    "    SubValid = fc[fc['Test']==0.5].head(1800)\n",
    "    SubTest = fc[fc['Test']==1]\n",
    "    SubDF = SubTrain.append(SubValid.append(SubTest))    \n",
    "    sub_df = SubDF.reset_index(drop=True)\n",
    "    return sub_df\n",
    "\n",
    "def create_tiny_df(full_chart):\n",
    "    '''This function creates a very small df, for debugging purposes\n",
    "    350 training observations, 100 validation, 50 test.\n",
    "    '''\n",
    "    training = full_chart.head(350)\n",
    "    test = full_chart.tail(50)\n",
    "    validation = full_chart[full_chart['Test']==0.5].head(100)\n",
    "\n",
    "    small_df = training.append(validation).append(test)\n",
    "    small_df = small_df.reset_index(drop=True)\n",
    "    return small_df\n",
    "\n",
    "def create_medium_df(full_chart):\n",
    "    '''This function creates a medium df as our backup option.\n",
    "    7000 training observations, 2000 validation, 1000 test.\n",
    "    '''\n",
    "    training = full_chart.head(7000)\n",
    "    test = full_chart.tail(1000)\n",
    "    validation = full_chart[full_chart['Test']==0.5].head(2000)\n",
    "    medium_df = training.append(validation).append(test)\n",
    "    medium_df = medium_df.reset_index(drop=True)\n",
    "    return medium_df\n",
    "\n",
    "\n",
    "def reduce_columns(df, version):\n",
    "    '''Remove unnecessary columns. Especially columns we may have used in analysis above, \n",
    "    but no longer need.'''\n",
    "    new_df = df[['HADM_ID', 'Test', 'Sentences-'+version, 'ICD9_Codes', 'ICD9_Codes_str']].copy()\n",
    "    return new_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tiny_PlainVocab = reduce_columns(create_tiny_df(Full_PlainVocab), 'PlainVocab')\n",
    "Tiny_Leven = reduce_columns(create_tiny_df(Full_Leven), 'Leven').drop('ICD9_Codes', 1)\n",
    "Tiny_Byte5K = reduce_columns(create_tiny_df(Full_Byte5K), 'Byte5K').drop('ICD9_Codes', 1)\n",
    "Tiny_Byte10K = reduce_columns(create_tiny_df(Full_Byte10K), 'Byte10K').drop('ICD9_Codes', 1)\n",
    "Tiny_Byte25K = reduce_columns(create_tiny_df(Full_Byte25K), 'Byte25K').drop('ICD9_Codes', 1)\n",
    "Tiny_Hybrid10K = reduce_columns(create_tiny_df(Full_Hybrid10K), 'Hybrid10K').drop('ICD9_Codes', 1)\n",
    "\n",
    "Med_PlainVocab = reduce_columns(create_medium_df(Full_PlainVocab), 'PlainVocab')\n",
    "Med_Leven = reduce_columns(create_medium_df(Full_Leven), 'Leven').drop('ICD9_Codes', 1)\n",
    "Med_Byte5K = reduce_columns(create_medium_df(Full_Byte5K), 'Byte5K').drop('ICD9_Codes', 1)\n",
    "Med_Byte10K = reduce_columns(create_medium_df(Full_Byte10K), 'Byte10K').drop('ICD9_Codes', 1)\n",
    "Med_Byte25K = reduce_columns(create_medium_df(Full_Byte25K), 'Byte25K').drop('ICD9_Codes', 1)\n",
    "Med_Hybrid10K = reduce_columns(create_medium_df(Full_Hybrid10K), 'Hybrid10K').drop('ICD9_Codes', 1)\n",
    "\n",
    "F_PlainVocab = reduce_columns(Full_PlainVocab, 'PlainVocab')\n",
    "F_Leven = reduce_columns(Full_Leven, 'Leven').drop('ICD9_Codes', 1)\n",
    "F_Byte5K = reduce_columns(Full_Byte5K, 'Byte5K').drop('ICD9_Codes', 1)\n",
    "F_Byte10K = reduce_columns(Full_Byte10K, 'Byte10K').drop('ICD9_Codes', 1)\n",
    "F_Byte25K = reduce_columns(Full_Byte25K, 'Byte25K').drop('ICD9_Codes', 1)\n",
    "F_Hybrid10K = reduce_columns(Full_Hybrid10K, 'Hybrid10K').drop('ICD9_Codes', 1)\n",
    "\n",
    "Sub_PlainVocab = reduce_columns(create_sub_df(Full_PlainVocab), 'PlainVocab')\n",
    "Sub_Leven = reduce_columns(create_sub_df(Full_Leven), 'Leven').drop('ICD9_Codes', 1)\n",
    "Sub_Byte5K = reduce_columns(create_sub_df(Full_Byte5K), 'Byte5K').drop('ICD9_Codes', 1)\n",
    "Sub_Byte10K = reduce_columns(create_sub_df(Full_Byte10K), 'Byte10K').drop('ICD9_Codes', 1)\n",
    "Sub_Byte25K = reduce_columns(create_sub_df(Full_Byte25K), 'Byte25K').drop('ICD9_Codes', 1)\n",
    "#Sub_Byte50K = reduce_columns(Full_Byte50K, 'Byte50K').drop('ICD9_Codes', 1)\n",
    "Sub_Hybrid10K = reduce_columns(create_sub_df(Full_Hybrid10K), 'Hybrid10K').drop('ICD9_Codes', 1)\n",
    "#Sub_Hybrid20K = reduce_columns(Full_Hybrid20K, 'Hybrid20K').drop('ICD9_Codes', 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''To make things easier, lets merge all these versions together.'''\n",
    "\n",
    "core_cols = ['HADM_ID', 'Test', 'ICD9_Codes_str']\n",
    "\n",
    "Tiny = pd.merge(Tiny_PlainVocab, Tiny_Leven.merge(Tiny_Byte5K.merge(Tiny_Byte10K.merge\n",
    "                (Tiny_Byte25K.merge(Tiny_Hybrid10K, on=core_cols), on=core_cols), on=core_cols),\n",
    "                on=core_cols), on=core_cols)\n",
    "\n",
    "Med = pd.merge(Med_PlainVocab, Med_Leven.merge(Med_Byte5K.merge(Med_Byte10K.merge\n",
    "                (Med_Byte25K.merge(Med_Hybrid10K, on=core_cols), on=core_cols), on=core_cols),\n",
    "                on=core_cols), on=core_cols)\n",
    "\n",
    "Full = pd.merge(F_PlainVocab, F_Leven.merge(F_Byte5K.merge(F_Byte10K.merge\n",
    "                (F_Byte25K.merge(F_Hybrid10K, on=core_cols), on=core_cols), on=core_cols),\n",
    "                on=core_cols), on=core_cols)\n",
    "\n",
    "Sub = pd.merge(Sub_PlainVocab, Sub_Leven.merge(Sub_Byte5K.merge(Sub_Byte10K.merge\n",
    "                (Sub_Byte25K.merge(Sub_Hybrid10K, on=core_cols), on=core_cols), on=core_cols),\n",
    "                on=core_cols), on=core_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>Sentences-PlainVocab</th>\n",
       "      <th>ICD9_Codes</th>\n",
       "      <th>ICD9_Codes_str</th>\n",
       "      <th>Sentences-Leven</th>\n",
       "      <th>Sentences-Byte5K</th>\n",
       "      <th>Sentences-Byte10K</th>\n",
       "      <th>Sentences-Byte25K</th>\n",
       "      <th>Sentences-Hybrid10K</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>6300</td>\n",
       "      <td>6300</td>\n",
       "      <td>6300</td>\n",
       "      <td>6300</td>\n",
       "      <td>6300</td>\n",
       "      <td>6300</td>\n",
       "      <td>6300</td>\n",
       "      <td>6300</td>\n",
       "      <td>6300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>887</td>\n",
       "      <td>887</td>\n",
       "      <td>887</td>\n",
       "      <td>887</td>\n",
       "      <td>887</td>\n",
       "      <td>887</td>\n",
       "      <td>887</td>\n",
       "      <td>887</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      HADM_ID  Sentences-PlainVocab  ICD9_Codes  ICD9_Codes_str  \\\n",
       "Test                                                              \n",
       "0.0      6300                  6300        6300            6300   \n",
       "0.5      1800                  1800        1800            1800   \n",
       "1.0       887                   887         887             887   \n",
       "\n",
       "      Sentences-Leven  Sentences-Byte5K  Sentences-Byte10K  Sentences-Byte25K  \\\n",
       "Test                                                                            \n",
       "0.0              6300              6300               6300               6300   \n",
       "0.5              1800              1800               1800               1800   \n",
       "1.0               887               887                887                887   \n",
       "\n",
       "      Sentences-Hybrid10K  \n",
       "Test                       \n",
       "0.0                  6300  \n",
       "0.5                  1800  \n",
       "1.0                   887  "
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sub['Length Sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34: codes to delete, full\n",
      "52: codes to delete, med\n",
      "39: codes to delete, tiny\n",
      "51: codes to delete, sub\n"
     ]
    }
   ],
   "source": [
    "#Remove ICD9 codes that appear ONLY in test set. \n",
    "\n",
    "def filter_out_test_set_codes(df):\n",
    "    '''This function removes codes that appear exclusively in the validation/test sets of a data frame.'''\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    train_set = new_df[new_df['Test']==0].copy()\n",
    "    test_set = new_df[new_df['Test']!=0].copy().reset_index()\n",
    "    training_codes = np.unique(np.concatenate(train_set['ICD9_Codes']))\n",
    "    testing_codes = np.unique(np.concatenate(test_set['ICD9_Codes']))\n",
    "    \n",
    "    codes_to_delete = [x for x in testing_codes if x not in training_codes]    \n",
    "    new_df['ICD9_Codes'] = new_df['ICD9_Codes'].map(lambda x: [code for code in x \n",
    "                                                          if code not in codes_to_delete])\n",
    "    new_df['ICD9_Codes_str'] = new_df['ICD9_Codes'].map(lambda x: ' '.join(x))\n",
    "    return new_df, codes_to_delete\n",
    "\n",
    "#------------------------------------------------\n",
    "\n",
    "Full_DF, Full_deleted_codes = filter_out_test_set_codes(Full)\n",
    "Med_DF, Med_deleted_codes = filter_out_test_set_codes(Med)\n",
    "Tiny_DF, Tiny_deleted_codes = filter_out_test_set_codes(Tiny)\n",
    "S_DF, Sub_deleted_codes = filter_out_test_set_codes(Sub)\n",
    "\n",
    "\n",
    "\n",
    "print(str(len(Full_deleted_codes)) + ': codes to delete, full')\n",
    "print(str(len(Med_deleted_codes)) + ': codes to delete, med')\n",
    "print(str(len(Tiny_deleted_codes)) + ': codes to delete, tiny')\n",
    "print(str(len(Sub_deleted_codes)) + ': codes to delete, sub')\n",
    "\n",
    "#print(deleted_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-Do:  find number of labels for each version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_binary_output(chart):\n",
    "    '''\n",
    "    Note: index of input df should be \"regular\" -- np.arange(len(chart))\n",
    "    @chart is the big dataframe (including summaries and ICD-9 codes)    \n",
    "        \n",
    "    This function returns the df, but creates a \"Labels\" column where each cell\n",
    "    contains a binary list (only 1's and 0's).\n",
    "\n",
    "    Each binary list is ~1000 elements long (representing the number of ICD9 Codes).\n",
    "    A \"1\" means the summary is associated with a ICD9 code, \"0\" otherwise.\n",
    "    \n",
    "    For example, if the only codes in the universe were 001, 002, 003, 004, and one summary \n",
    "    was associated with 002 and 004, the binary list for that summary would be [0, 1, 0, 1]\n",
    "    '''\n",
    "    \n",
    "    import sklearn\n",
    "    from sklearn import feature_extraction\n",
    "    VectorizerCodes = sklearn.feature_extraction.text.CountVectorizer()\n",
    "    \n",
    "    matrix = VectorizerCodes.fit_transform(chart['ICD9_Codes_str'])\n",
    "    colnames = VectorizerCodes.get_feature_names()\n",
    "    array_out = np.array(matrix.toarray(), dtype=np.float32)    \n",
    "    Output = pd.DataFrame(array_out, columns = colnames, index = chart.index) #copy index from large table.\n",
    "    \n",
    "    Output['Labels'] = Output.index.map(lambda x: list(Output.loc[x,:]))\n",
    "    O = Output[['Labels']]\n",
    "    new_chart = pd.merge(chart, O, right_index=True, left_index=True)\n",
    "    return new_chart, colnames, array_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Full_FinalDF, FullCodes, f = generate_binary_output(Full_DF)\n",
    "#Med_FinalDF, MedCodes, _ = generate_binary_output(Med_DF)\n",
    "#Tiny_FinalDF, TinyCodes, x = generate_binary_output(Tiny_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29683, 937)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountPerCode = {}\n",
    "for c in range(f.shape[1]):\n",
    "    if c % 100 == 0:\n",
    "        print(c)\n",
    "    code = f.columns.values[c]\n",
    "    CountPerCode[code] = f[code].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['401', '486', '557', '560', '997']\n",
      "124\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#Double-checking work\n",
    "print(Tiny_FinalDF.loc[0,'ICD9_Codes'])\n",
    "print(TinyCodes.index('401'))\n",
    "print(Tiny_FinalDF.loc[0,'Labels'][124])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_vocabulary_size(chart, column_name):\n",
    "    '''\n",
    "    chart is a pd DataFrame\n",
    "    This function finds the size of the vocabulary for a particular column.\n",
    "    column_name should either be \"Summary-PlainVocab\" or \"Summary-Byte50K\"\n",
    "    '''\n",
    "    return len(np.unique(np.concatenate(chart[column_name].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_vocabulary_size(DFx, 'Sentences-Leven')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_valid_test(chart):\n",
    "    '''Takes a df and splits it into train, test, and validation sets based on \n",
    "    predetermined column.'''\n",
    "    train = chart[chart['Test']==0]\n",
    "    validation = chart[chart['Test']==0.5]\n",
    "    test = chart[chart['Test']==1]\n",
    "    \n",
    "    train = train.drop(['Test', 'HADM_ID', 'ICD9_Codes', 'ICD9_Codes_str'], 1)\n",
    "    validation = validation.drop(['Test', 'HADM_ID', 'ICD9_Codes', 'ICD9_Codes_str'], 1)\n",
    "    test = test.drop(['Test', 'HADM_ID', 'ICD9_Codes', 'ICD9_Codes_str'], 1)\n",
    "    \n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tiny_Train, Tiny_Validation, Tiny_Test = split_train_valid_test(Tiny_FinalDF)\n",
    "Med_Train, Med_Validation, Med_Test = split_train_valid_test(Med_FinalDF)\n",
    "Full_Train, Full_Validation, Full_Test = split_train_valid_test(Full_FinalDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29683"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Full_FinalDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save chart(s) as pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save_as_pkl(Tiny_Train, './TextDFs/Tiny/TinyTrain')\n",
    "#save_as_pkl(Tiny_Validation, './TextDFs/Tiny/TinyValidation')\n",
    "#save_as_pkl(Tiny_Test, './TextDFs/Tiny/TinyTest')\n",
    "#save_as_pkl(Med_Train, './TextDFs/Med/MedTrain')\n",
    "#save_as_pkl(Med_Validation, './TextDFs/Med/MedValidation')\n",
    "#save_as_pkl(Med_Test, './TextDFs/Med/MedTest')\n",
    "#save_as_pkl(Full_Train, './TextDFs/Full/FullTrain')\n",
    "#save_as_pkl(Full_Validation, './TextDFs/Full/FullValidation')\n",
    "#save_as_pkl(Full_Test, './TextDFs/Full/FullTest')\n",
    "\n",
    "save_as_pkl(Tiny_Train, './NewDFs/TinyTrain')\n",
    "save_as_pkl(Tiny_Validation, './NewDFs/TinyValidation')\n",
    "save_as_pkl(Tiny_Test, './NewDFs/TinyTest')\n",
    "save_as_pkl(Med_Train, './NewDFs/MedTrain')\n",
    "save_as_pkl(Med_Validation, './NewDFs/MedValidation')\n",
    "save_as_pkl(Med_Test, './NewDFs/MedTest')\n",
    "save_as_pkl(Full_Train, './NewDFs/FullTrain')\n",
    "save_as_pkl(Full_Validation, './NewDFs/FullValidation')\n",
    "save_as_pkl(Full_Test, './NewDFs/FullTest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
