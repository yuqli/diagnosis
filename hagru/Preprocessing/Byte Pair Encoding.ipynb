{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, collections \n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_numbers(vocab_list):\n",
    "    '''This function assigns index numbers to words in a vocabulary.'''\n",
    "    vocab_indexer = dict()\n",
    "    index_value = 2\n",
    "    for word in vocab_list:\n",
    "        vocab_indexer[word] = index_value\n",
    "        index_value = index_value+1\n",
    "    return vocab_indexer\n",
    "\n",
    "def token_to_index(tokens, token_indexer):\n",
    "    \"\"\"\n",
    "    Function that transforms a list of tokens in a document and coverts it to a list of token indices.\n",
    "    @param tokens: list of tokens from one document\n",
    "    @param token_indexer: dictionary that maps each token in the vocabulary to an unique index\n",
    "    \"\"\"\n",
    "    # Please DO NOT assign any ngram in the vocabulary to index 0\n",
    "    document = []\n",
    "    for token in tokens:\n",
    "        try:  \n",
    "            document.append(token_indexer[token])\n",
    "        except:\n",
    "            document.append(1)\n",
    "    return document\n",
    "\n",
    "\n",
    "def split_tokens_into_characters(counter):\n",
    "    '''This function takes a counter (indicating the counts per token across the entire corpus) and \n",
    "    splits each token into a series of characters'''\n",
    "    char_dict = {}\n",
    "    old_dict = dict(counter)\n",
    "    for token in old_dict.keys():\n",
    "        char_form = \" \".join(token)+' </w>' #add ending character\n",
    "        value = old_dict[token]\n",
    "        char_dict[char_form] = value \n",
    "    return char_dict\n",
    "\n",
    "def get_stats(vocab):\n",
    "    '''This function counts the number of times each pair of characters appears next to each other.\n",
    "    For example the sentence \"baking a cake\" would return (a,k):2, (b,a):1, (k,e):1, and so on.\n",
    "    '''\n",
    "    pairs = collections.defaultdict(int) \n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq \n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    '''This function changes the vocabulary so that n-grams that co-occur together frequently\n",
    "    are merged'''\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word] \n",
    "    return v_out\n",
    "\n",
    "def convert_tokens_into_frequent_character_combinations(counter, num_merges):\n",
    "    '''This function takes a Counter object (counts per token) and converts it so that \n",
    "    the tokens (keys) become split into frequent character combinations.\n",
    "    For example, {'cake': 3, 'baking': 1} might become {'c ak e': 3, 'b ak ing': 1}\n",
    "    \n",
    "    @num_merges is the number of \"rounds\" for merging character blocks. For each round, the \n",
    "    pair of characters that co-occur the most are combined. \n",
    "    \n",
    "    @num_merges is a very important hyperparameter.\n",
    "    '''\n",
    "    char_dict = split_tokens_into_characters(counter)\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(char_dict)\n",
    "        best = max(pairs, key=pairs.get) \n",
    "        char_dict = merge_vocab(best, char_dict)\n",
    "        print('Sample: '+ str(char_dict))\n",
    "    return char_dict, pairs\n",
    "\n",
    "def create_vocabulary_of_character_combos(char_dict):\n",
    "    '''Takes the character_dictionary from above and creates an indexed vocabulary \n",
    "    for frequent character combinations.\n",
    "    \n",
    "    For example, {'c ak e': 3, 'b ak ing': 1} would become something like\n",
    "    {'c': 1, 'b': 2, 'ak': 3, 'e': 4, 'ing': 5}. \n",
    "    \n",
    "    Eventually we want to map each unigram to a series of character blocks \n",
    "    (e.g. 'baking' becomes 2 3 5)\n",
    "    '''\n",
    "    #First, find the set of all grams\n",
    "    grams = []\n",
    "    for word in char_dict.keys():\n",
    "        word_grams = word.split(' ')\n",
    "        grams.extend(word_grams)\n",
    "    GRAMS = list(np.unique(grams)) #Delete any duplicates\n",
    "    Vocab_Index = assign_numbers(GRAMS)\n",
    "    return Vocab_Index\n",
    "\n",
    "def map_unigram_to_character_indices(char_dict, Vocab_Index):\n",
    "    '''This function maps each unigram token to a series of numbers. Each number corresponds to a \n",
    "    character block.\n",
    "    \n",
    "    For example, if Vocab_Index = {'c': 1, 'b': 2, 'ak': 3, 'e': 4, 'ing': 5}, \n",
    "    \"baking\" should be mapped to [2, 3, 5]\n",
    "    \n",
    "    \n",
    "    @char_dict is the counter-like object where the keys are unigrams split into character blocks, \n",
    "    values are the number of times the unigram appears.\n",
    "    '''\n",
    "    unigrams = {} \n",
    "    for x in char_dict.keys():\n",
    "        unigram = x.replace(' ','').replace('</w>','') #e.g. revert \"b ak ing<w>\" to baking\n",
    "        unigrams[unigram] = x\n",
    "    \n",
    "    for w in unigrams.keys():\n",
    "        unigrams[w] = token_to_index(unigrams[w].split(' '), Vocab_Index)\n",
    "    return unigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_cts = {'low' : 5, 'lower' : 2, 'newest':6,'widest':3} #e.g. \"low\" appears 5 times in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: {'l o w </w>': 5, 'w i d es t </w>': 3, 'l o w e r </w>': 2, 'n e w es t </w>': 6}\n",
      "Sample: {'l o w e r </w>': 2, 'n e w est </w>': 6, 'l o w </w>': 5, 'w i d est </w>': 3}\n",
      "Sample: {'n e w est</w>': 6, 'w i d est</w>': 3, 'l o w e r </w>': 2, 'l o w </w>': 5}\n",
      "Sample: {'l ow e r </w>': 2, 'w i d est</w>': 3, 'n e w est</w>': 6, 'l ow </w>': 5}\n",
      "Sample: {'low </w>': 5, 'w i d est</w>': 3, 'n e w est</w>': 6, 'low e r </w>': 2}\n",
      "Sample: {'ne w est</w>': 6, 'w i d est</w>': 3, 'low </w>': 5, 'low e r </w>': 2}\n",
      "Sample: {'new est</w>': 6, 'low </w>': 5, 'w i d est</w>': 3, 'low e r </w>': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird_cts, pairs = convert_tokens_into_frequent_character_combinations(old_cts, 7)\n",
    "weird_cts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('d', 'est</w>'): 3,\n",
       "             ('e', 'r'): 2,\n",
       "             ('i', 'd'): 3,\n",
       "             ('low', '</w>'): 5,\n",
       "             ('low', 'e'): 2,\n",
       "             ('ne', 'w'): 6,\n",
       "             ('r', '</w>'): 2,\n",
       "             ('w', 'est</w>'): 6,\n",
       "             ('w', 'i'): 3})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just an example. If we were to do one more round, we would either merge \"ne\" and \"w\" to \"new,\n",
    "#or merge \"w\" and \"est</w>\" to \"west</w>\"\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'</w>': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'est</w>': 5,\n",
       " 'i': 6,\n",
       " 'low': 7,\n",
       " 'new': 8,\n",
       " 'r': 9,\n",
       " 'w': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vocab = create_vocabulary_of_character_combos(weird_cts)\n",
    "char_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'low': [7, 2],\n",
       " 'lower': [7, 4, 9, 2],\n",
       " 'newest': [8, 5],\n",
       " 'widest': [10, 6, 3, 5]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unigram_to_character_indices(weird_cts, char_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
